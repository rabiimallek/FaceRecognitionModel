{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xiq9KpfxGB7J",
        "outputId": "c7b86d83-4d31-4176-acc2-b05c1660bb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW4TPeDU7N1J"
      },
      "source": [
        "# Prétraitement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERPbDjET7TK7",
        "outputId": "b75fd7d5-39d9-4c9a-9751-ce0f55697b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le dossier '/content/drive/MyDrive/Expériementation_Adience_DB/train_data' contient 15651 images.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Chemin d'accès au dossier contenant les images\n",
        "path = \"/content/drive/MyDrive/Expériementation_Adience_DB/train_data\"\n",
        "\n",
        "# Compter le nombre d'images\n",
        "count = 0\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".jpg\") or (\".png\"):\n",
        "            count += 1\n",
        "\n",
        "# Afficher le résultat\n",
        "print(f\"Le dossier '{path}' contient {count} images.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6eKI1e07ZUt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# chemin du dossier contenant les images\n",
        "folder_path = '/content/drive/MyDrive/Expériementation_Adience_DB/train_data'\n",
        "\n",
        "# Liste des noms de tous les sous-dossiers\n",
        "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "# Créer une liste vide pour stocker les tableaux d'images\n",
        "X_train = []\n",
        "# Créer une liste vide pour stocker les étiquettes\n",
        "X_labels = []\n",
        "\n",
        "# Parcourir chaque sous-dossier\n",
        "for i, subfolder in enumerate(subfolders):\n",
        "    # Liste des fichiers dans le sous-dossier\n",
        "    file_names = [f.path for f in os.scandir(subfolder) if f.is_file()]\n",
        "    # Parcourir chaque fichier\n",
        "    for file_name in file_names:\n",
        "        # Ouvrir l'image avec PIL\n",
        "        image = Image.open(file_name)\n",
        "        image = image.convert('RGB')\n",
        "        # Redimensionner l'image à (299, 299)\n",
        "        image = image.resize((64,64))\n",
        "        # Convertir l'image en tableau numpy\n",
        "        image_array = np.array(image)\n",
        "        # Ajouter le tableau d'image à la liste\n",
        "        X_train.append(image_array)\n",
        "        # Ajouter l'étiquette à la liste\n",
        "        X_labels.append(i)\n",
        "\n",
        "X_train = np.array(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHGOhVjT7bkw",
        "outputId": "e03377cb-a4aa-4bfa-caac-6a01c0116202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15651\n",
            "15651\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train))\n",
        "print(len(X_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF1uMkzz7b7Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# chemin du dossier contenant les images\n",
        "folder_path = '/content/drive/MyDrive/Expériementation_Adience_DB/test_data'\n",
        "\n",
        "# Liste des noms de tous les sous-dossiers\n",
        "subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "# Créer une liste vide pour stocker les tableaux d'images\n",
        "Y_test = []\n",
        "# Créer une liste vide pour stocker les étiquettes\n",
        "Y_labels= []\n",
        "\n",
        "# Parcourir chaque sous-dossier\n",
        "for i, subfolder in enumerate(subfolders):\n",
        "    # Liste des fichiers dans le sous-dossier\n",
        "    file_names = [f.path for f in os.scandir(subfolder) if f.is_file()]\n",
        "    # Parcourir chaque fichier\n",
        "    for file_name in file_names:\n",
        "        # Ouvrir l'image avec PIL\n",
        "        image = Image.open(file_name)\n",
        "        image = image.convert('RGB')\n",
        "        # Redimensionner l'image à (299, 299)\n",
        "        image = image.resize((64,64))\n",
        "        # Convertir l'image en tableau numpy\n",
        "        image_array = np.array(image)\n",
        "        # Ajouter le tableau d'image à la liste\n",
        "        Y_test.append(image_array)\n",
        "        # Ajouter l'étiquette à la liste\n",
        "        Y_labels.append(i)\n",
        "\n",
        "\n",
        "Y_test = np.array(Y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM33JdvN77tJ",
        "outputId": "3af5278b-85f6-4b81-fe99-3b8d6c959317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3716\n",
            "3716\n"
          ]
        }
      ],
      "source": [
        "print(len(Y_labels))\n",
        "print(len(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRlCTkPMSUwD",
        "outputId": "82a41f61-53ce-4bd8-fe4b-e6cbe7a46b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3716\n",
            "3716\n",
            "15651\n",
            "15651\n"
          ]
        }
      ],
      "source": [
        "print(len(Y_labels))\n",
        "print(len(Y_test))\n",
        "print(len(X_train))\n",
        "print(len(X_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LV7G4yI8AXR"
      },
      "source": [
        "# VGG19\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6iErB8I8EaG",
        "outputId": "598a815b-d368-4e79-baeb-d51f0308c4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# importing the libraries\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.applications import vgg19\n",
        "from keras.layers import Dropout,Dense,Conv2D,GlobalAveragePooling2D\n",
        "#from keras.preprocessing import image\n",
        "num_classes=2284\n",
        "IMAGE_SIZE = [64,64]  \n",
        "\n",
        "# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n",
        "vgg19 = vgg19.VGG19(input_shape = IMAGE_SIZE + [3],weights='imagenet',include_top=False)\n",
        "\n",
        "# this will exclude the initial layers from training phase as there are already been trained.\n",
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False\n",
        "x = Flatten()(vgg19.output)\n",
        "#x1 = Dense(512, activation = 'relu')(x)   # we can add a new fully connected layer but it will increase the execution time.\n",
        "output_layer = Dense(num_classes, activation = 'softmax')(x)  # adding the output layer with softmax function as this is a multi label classification problem.\n",
        "\n",
        "model = Model(inputs = vgg19.input, outputs = output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQg_DNiP8Gyu",
        "outputId": "63994f4a-8182-4fc7-dc06-d91d224150f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2284)              4679916   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,704,300\n",
            "Trainable params: 4,679,916\n",
            "Non-trainable params: 20,024,384\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSQiMr1l8JAB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmGn_COCHCwR"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train)\n",
        "X_labels = np.array(X_labels)\n",
        "Y_test = np.array(Y_test)\n",
        "Y_labels = np.array(Y_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoY9yx-H8KSH",
        "outputId": "3754e956-249b-4bef-cef3-991f527aa921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "157/157 [==============================] - 22s 72ms/step - loss: 32.3814 - accuracy: 0.0100 - val_loss: 24.9419 - val_accuracy: 0.0237\n",
            "Epoch 2/1000\n",
            "157/157 [==============================] - 9s 60ms/step - loss: 20.0172 - accuracy: 0.0496 - val_loss: 19.7738 - val_accuracy: 0.0562\n",
            "Epoch 3/1000\n",
            "157/157 [==============================] - 9s 60ms/step - loss: 13.4980 - accuracy: 0.1268 - val_loss: 16.7703 - val_accuracy: 0.0912\n",
            "Epoch 4/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.1168 - accuracy: 0.2376 - val_loss: 14.9007 - val_accuracy: 0.1267\n",
            "Epoch 5/1000\n",
            "157/157 [==============================] - 10s 67ms/step - loss: 6.1471 - accuracy: 0.3604 - val_loss: 13.7038 - val_accuracy: 0.1577\n",
            "Epoch 6/1000\n",
            "157/157 [==============================] - 10s 62ms/step - loss: 4.1861 - accuracy: 0.4928 - val_loss: 12.9105 - val_accuracy: 0.1878\n",
            "Epoch 7/1000\n",
            "157/157 [==============================] - 11s 68ms/step - loss: 2.8898 - accuracy: 0.5987 - val_loss: 12.3579 - val_accuracy: 0.2080\n",
            "Epoch 8/1000\n",
            "157/157 [==============================] - 11s 68ms/step - loss: 2.0456 - accuracy: 0.6877 - val_loss: 12.0533 - val_accuracy: 0.2204\n",
            "Epoch 9/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 1.4707 - accuracy: 0.7541 - val_loss: 11.7468 - val_accuracy: 0.2379\n",
            "Epoch 10/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 1.0751 - accuracy: 0.8122 - val_loss: 11.5808 - val_accuracy: 0.2495\n",
            "Epoch 11/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.7928 - accuracy: 0.8537 - val_loss: 11.4510 - val_accuracy: 0.2583\n",
            "Epoch 12/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.5940 - accuracy: 0.8873 - val_loss: 11.3581 - val_accuracy: 0.2664\n",
            "Epoch 13/1000\n",
            "157/157 [==============================] - 11s 68ms/step - loss: 0.4454 - accuracy: 0.9123 - val_loss: 11.2919 - val_accuracy: 0.2713\n",
            "Epoch 14/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.3392 - accuracy: 0.9336 - val_loss: 11.2307 - val_accuracy: 0.2734\n",
            "Epoch 15/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.2588 - accuracy: 0.9507 - val_loss: 11.1954 - val_accuracy: 0.2826\n",
            "Epoch 16/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.1989 - accuracy: 0.9635 - val_loss: 11.1482 - val_accuracy: 0.2842\n",
            "Epoch 17/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.1532 - accuracy: 0.9735 - val_loss: 11.1269 - val_accuracy: 0.2839\n",
            "Epoch 18/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.1202 - accuracy: 0.9809 - val_loss: 11.1106 - val_accuracy: 0.2890\n",
            "Epoch 19/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0958 - accuracy: 0.9856 - val_loss: 11.0702 - val_accuracy: 0.2914\n",
            "Epoch 20/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0761 - accuracy: 0.9893 - val_loss: 11.0478 - val_accuracy: 0.2952\n",
            "Epoch 21/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0616 - accuracy: 0.9925 - val_loss: 11.0342 - val_accuracy: 0.2944\n",
            "Epoch 22/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0503 - accuracy: 0.9946 - val_loss: 11.0214 - val_accuracy: 0.2966\n",
            "Epoch 23/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0410 - accuracy: 0.9958 - val_loss: 11.0126 - val_accuracy: 0.3014\n",
            "Epoch 24/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0348 - accuracy: 0.9970 - val_loss: 10.9994 - val_accuracy: 0.3019\n",
            "Epoch 25/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0285 - accuracy: 0.9984 - val_loss: 10.9849 - val_accuracy: 0.3054\n",
            "Epoch 26/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0247 - accuracy: 0.9987 - val_loss: 10.9807 - val_accuracy: 0.3052\n",
            "Epoch 27/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0208 - accuracy: 0.9992 - val_loss: 10.9627 - val_accuracy: 0.3071\n",
            "Epoch 28/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0187 - accuracy: 0.9993 - val_loss: 10.9545 - val_accuracy: 0.3076\n",
            "Epoch 29/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0163 - accuracy: 0.9995 - val_loss: 10.9500 - val_accuracy: 0.3119\n",
            "Epoch 30/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0142 - accuracy: 0.9996 - val_loss: 10.9437 - val_accuracy: 0.3122\n",
            "Epoch 31/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0128 - accuracy: 0.9997 - val_loss: 10.9265 - val_accuracy: 0.3151\n",
            "Epoch 32/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0117 - accuracy: 0.9997 - val_loss: 10.9321 - val_accuracy: 0.3178\n",
            "Epoch 33/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0108 - accuracy: 0.9997 - val_loss: 10.9192 - val_accuracy: 0.3167\n",
            "Epoch 34/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0096 - accuracy: 0.9997 - val_loss: 10.9145 - val_accuracy: 0.3197\n",
            "Epoch 35/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0088 - accuracy: 0.9997 - val_loss: 10.8984 - val_accuracy: 0.3216\n",
            "Epoch 36/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 10.8988 - val_accuracy: 0.3213\n",
            "Epoch 37/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0075 - accuracy: 0.9997 - val_loss: 10.8971 - val_accuracy: 0.3221\n",
            "Epoch 38/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0065 - accuracy: 0.9998 - val_loss: 10.8867 - val_accuracy: 0.3245\n",
            "Epoch 39/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 10.8853 - val_accuracy: 0.3264\n",
            "Epoch 40/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0057 - accuracy: 0.9998 - val_loss: 10.8774 - val_accuracy: 0.3278\n",
            "Epoch 41/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0054 - accuracy: 0.9999 - val_loss: 10.8750 - val_accuracy: 0.3270\n",
            "Epoch 42/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0056 - accuracy: 0.9998 - val_loss: 10.8696 - val_accuracy: 0.3280\n",
            "Epoch 43/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0047 - accuracy: 0.9999 - val_loss: 10.8693 - val_accuracy: 0.3318\n",
            "Epoch 44/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 10.8626 - val_accuracy: 0.3315\n",
            "Epoch 45/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0040 - accuracy: 0.9999 - val_loss: 10.8644 - val_accuracy: 0.3315\n",
            "Epoch 46/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0044 - accuracy: 0.9998 - val_loss: 10.8568 - val_accuracy: 0.3329\n",
            "Epoch 47/1000\n",
            "157/157 [==============================] - 11s 68ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 10.8610 - val_accuracy: 0.3332\n",
            "Epoch 48/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 10.8548 - val_accuracy: 0.3367\n",
            "Epoch 49/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 10.8370 - val_accuracy: 0.3358\n",
            "Epoch 50/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0036 - accuracy: 0.9997 - val_loss: 10.8473 - val_accuracy: 0.3375\n",
            "Epoch 51/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 10.8355 - val_accuracy: 0.3372\n",
            "Epoch 52/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0035 - accuracy: 0.9998 - val_loss: 10.8318 - val_accuracy: 0.3377\n",
            "Epoch 53/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 10.8256 - val_accuracy: 0.3377\n",
            "Epoch 54/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 10.8288 - val_accuracy: 0.3388\n",
            "Epoch 55/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 10.8350 - val_accuracy: 0.3415\n",
            "Epoch 56/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 10.8206 - val_accuracy: 0.3418\n",
            "Epoch 57/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 10.8239 - val_accuracy: 0.3412\n",
            "Epoch 58/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 10.8242 - val_accuracy: 0.3420\n",
            "Epoch 59/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 10.8180 - val_accuracy: 0.3434\n",
            "Epoch 60/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 10.8134 - val_accuracy: 0.3458\n",
            "Epoch 61/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 10.8164 - val_accuracy: 0.3458\n",
            "Epoch 62/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 10.8084 - val_accuracy: 0.3474\n",
            "Epoch 63/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 10.8132 - val_accuracy: 0.3477\n",
            "Epoch 64/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 10.8110 - val_accuracy: 0.3480\n",
            "Epoch 65/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 10.8156 - val_accuracy: 0.3490\n",
            "Epoch 66/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 10.8010 - val_accuracy: 0.3501\n",
            "Epoch 67/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 10.8118 - val_accuracy: 0.3525\n",
            "Epoch 68/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 10.8039 - val_accuracy: 0.3517\n",
            "Epoch 69/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 10.7996 - val_accuracy: 0.3528\n",
            "Epoch 70/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 10.8048 - val_accuracy: 0.3547\n",
            "Epoch 71/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 10.7914 - val_accuracy: 0.3568\n",
            "Epoch 72/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 10.7989 - val_accuracy: 0.3547\n",
            "Epoch 73/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 10.8022 - val_accuracy: 0.3574\n",
            "Epoch 74/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 10.8000 - val_accuracy: 0.3584\n",
            "Epoch 75/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 10.7911 - val_accuracy: 0.3609\n",
            "Epoch 76/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 11.1014 - val_accuracy: 0.3509\n",
            "Epoch 77/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.1979 - accuracy: 0.9514 - val_loss: 11.7741 - val_accuracy: 0.3369\n",
            "Epoch 78/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.1053 - accuracy: 0.9732 - val_loss: 11.2848 - val_accuracy: 0.3636\n",
            "Epoch 79/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0185 - accuracy: 0.9953 - val_loss: 11.0348 - val_accuracy: 0.3751\n",
            "Epoch 80/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 10.9454 - val_accuracy: 0.3800\n",
            "Epoch 81/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 10.9217 - val_accuracy: 0.3800\n",
            "Epoch 82/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 10.9158 - val_accuracy: 0.3811\n",
            "Epoch 83/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 10.9113 - val_accuracy: 0.3808\n",
            "Epoch 84/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 10.9102 - val_accuracy: 0.3813\n",
            "Epoch 85/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 10.9111 - val_accuracy: 0.3827\n",
            "Epoch 86/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 10.9184 - val_accuracy: 0.3835\n",
            "Epoch 87/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 10.9082 - val_accuracy: 0.3821\n",
            "Epoch 88/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 10.9200 - val_accuracy: 0.3835\n",
            "Epoch 89/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 10.9155 - val_accuracy: 0.3843\n",
            "Epoch 90/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 10.9252 - val_accuracy: 0.3854\n",
            "Epoch 91/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 10.9186 - val_accuracy: 0.3851\n",
            "Epoch 92/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 10.9124 - val_accuracy: 0.3854\n",
            "Epoch 93/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 10.9125 - val_accuracy: 0.3862\n",
            "Epoch 94/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 10.9263 - val_accuracy: 0.3875\n",
            "Epoch 95/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 10.9215 - val_accuracy: 0.3881\n",
            "Epoch 96/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 10.9270 - val_accuracy: 0.3883\n",
            "Epoch 97/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 10.9258 - val_accuracy: 0.3891\n",
            "Epoch 98/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 10.9295 - val_accuracy: 0.3905\n",
            "Epoch 99/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 10.9273 - val_accuracy: 0.3902\n",
            "Epoch 100/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 10.9205 - val_accuracy: 0.3913\n",
            "Epoch 101/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 10.9293 - val_accuracy: 0.3926\n",
            "Epoch 102/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 10.9276 - val_accuracy: 0.3926\n",
            "Epoch 103/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 10.9316 - val_accuracy: 0.3921\n",
            "Epoch 104/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 10.9473 - val_accuracy: 0.3924\n",
            "Epoch 105/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 10.9351 - val_accuracy: 0.3924\n",
            "Epoch 106/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 10.9419 - val_accuracy: 0.3926\n",
            "Epoch 107/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 10.9403 - val_accuracy: 0.3932\n",
            "Epoch 108/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 10.9467 - val_accuracy: 0.3937\n",
            "Epoch 109/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 10.9468 - val_accuracy: 0.3948\n",
            "Epoch 110/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 10.9505 - val_accuracy: 0.3948\n",
            "Epoch 111/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 10.9587 - val_accuracy: 0.3953\n",
            "Epoch 112/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 10.9576 - val_accuracy: 0.3937\n",
            "Epoch 113/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 10.9583 - val_accuracy: 0.3953\n",
            "Epoch 114/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 10.9579 - val_accuracy: 0.3956\n",
            "Epoch 115/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 10.9558 - val_accuracy: 0.3959\n",
            "Epoch 116/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 10.9627 - val_accuracy: 0.3969\n",
            "Epoch 117/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 6.2341e-04 - accuracy: 0.9999 - val_loss: 10.9741 - val_accuracy: 0.3948\n",
            "Epoch 118/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 10.9624 - val_accuracy: 0.3967\n",
            "Epoch 119/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 10.9747 - val_accuracy: 0.3964\n",
            "Epoch 120/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 10.9684 - val_accuracy: 0.3977\n",
            "Epoch 121/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 10.9837 - val_accuracy: 0.3980\n",
            "Epoch 122/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 10.9756 - val_accuracy: 0.3969\n",
            "Epoch 123/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 10.9874 - val_accuracy: 0.3975\n",
            "Epoch 124/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 10.9982 - val_accuracy: 0.3977\n",
            "Epoch 125/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 10.9961 - val_accuracy: 0.3980\n",
            "Epoch 126/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 10.9842 - val_accuracy: 0.3996\n",
            "Epoch 127/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.0088 - val_accuracy: 0.3977\n",
            "Epoch 128/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.0060 - val_accuracy: 0.4012\n",
            "Epoch 129/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.0100 - val_accuracy: 0.4007\n",
            "Epoch 130/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.0016 - val_accuracy: 0.3999\n",
            "Epoch 131/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.0207 - val_accuracy: 0.4004\n",
            "Epoch 132/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.0074 - val_accuracy: 0.4007\n",
            "Epoch 133/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.0217 - val_accuracy: 0.3994\n",
            "Epoch 134/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.0283 - val_accuracy: 0.3994\n",
            "Epoch 135/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.0319 - val_accuracy: 0.3991\n",
            "Epoch 136/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.0392 - val_accuracy: 0.4007\n",
            "Epoch 137/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.0402 - val_accuracy: 0.4010\n",
            "Epoch 138/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.0474 - val_accuracy: 0.3996\n",
            "Epoch 139/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.0488 - val_accuracy: 0.4002\n",
            "Epoch 140/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.0550 - val_accuracy: 0.4015\n",
            "Epoch 141/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.8643e-04 - accuracy: 0.9999 - val_loss: 11.0587 - val_accuracy: 0.4020\n",
            "Epoch 142/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.0620 - val_accuracy: 0.4020\n",
            "Epoch 143/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.0697 - val_accuracy: 0.4018\n",
            "Epoch 144/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 11.0686 - val_accuracy: 0.4034\n",
            "Epoch 145/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.0588 - val_accuracy: 0.4029\n",
            "Epoch 146/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.0830 - val_accuracy: 0.4039\n",
            "Epoch 147/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0547 - accuracy: 0.9855 - val_loss: 12.2497 - val_accuracy: 0.3727\n",
            "Epoch 148/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0385 - accuracy: 0.9895 - val_loss: 11.7779 - val_accuracy: 0.3870\n",
            "Epoch 149/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 11.4943 - val_accuracy: 0.4002\n",
            "Epoch 150/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.3992 - val_accuracy: 0.4045\n",
            "Epoch 151/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.5200e-04 - accuracy: 0.9999 - val_loss: 11.3917 - val_accuracy: 0.4072\n",
            "Epoch 152/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.4268 - val_accuracy: 0.4058\n",
            "Epoch 153/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 11.3896 - val_accuracy: 0.4058\n",
            "Epoch 154/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.3786 - val_accuracy: 0.4072\n",
            "Epoch 155/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.3753 - val_accuracy: 0.4096\n",
            "Epoch 156/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.3705 - val_accuracy: 0.4093\n",
            "Epoch 157/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.3680 - val_accuracy: 0.4101\n",
            "Epoch 158/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.3722 - val_accuracy: 0.4096\n",
            "Epoch 159/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.3804 - val_accuracy: 0.4104\n",
            "Epoch 160/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.3802 - val_accuracy: 0.4101\n",
            "Epoch 161/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 11.3829 - val_accuracy: 0.4112\n",
            "Epoch 162/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.3907 - val_accuracy: 0.4107\n",
            "Epoch 163/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.3828 - val_accuracy: 0.4109\n",
            "Epoch 164/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 11.3823 - val_accuracy: 0.4120\n",
            "Epoch 165/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.3891 - val_accuracy: 0.4120\n",
            "Epoch 166/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.3912 - val_accuracy: 0.4123\n",
            "Epoch 167/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.3915 - val_accuracy: 0.4123\n",
            "Epoch 168/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 11.3987 - val_accuracy: 0.4120\n",
            "Epoch 169/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.4006 - val_accuracy: 0.4112\n",
            "Epoch 170/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.4021 - val_accuracy: 0.4131\n",
            "Epoch 171/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.4049 - val_accuracy: 0.4128\n",
            "Epoch 172/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.4140 - val_accuracy: 0.4131\n",
            "Epoch 173/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.4095 - val_accuracy: 0.4128\n",
            "Epoch 174/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.4165 - val_accuracy: 0.4142\n",
            "Epoch 175/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 11.4182 - val_accuracy: 0.4152\n",
            "Epoch 176/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.4217 - val_accuracy: 0.4152\n",
            "Epoch 177/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.4210 - val_accuracy: 0.4152\n",
            "Epoch 178/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 7.7233e-04 - accuracy: 0.9999 - val_loss: 11.4308 - val_accuracy: 0.4158\n",
            "Epoch 179/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.4237 - val_accuracy: 0.4155\n",
            "Epoch 180/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.4244 - val_accuracy: 0.4171\n",
            "Epoch 181/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.4349 - val_accuracy: 0.4155\n",
            "Epoch 182/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.4392 - val_accuracy: 0.4166\n",
            "Epoch 183/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.4382 - val_accuracy: 0.4160\n",
            "Epoch 184/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.4429 - val_accuracy: 0.4166\n",
            "Epoch 185/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.4500 - val_accuracy: 0.4155\n",
            "Epoch 186/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.4661 - val_accuracy: 0.4166\n",
            "Epoch 187/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.4568 - val_accuracy: 0.4158\n",
            "Epoch 188/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 11.4535 - val_accuracy: 0.4174\n",
            "Epoch 189/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.4747 - val_accuracy: 0.4174\n",
            "Epoch 190/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.4744 - val_accuracy: 0.4171\n",
            "Epoch 191/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.4714 - val_accuracy: 0.4177\n",
            "Epoch 192/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.4710 - val_accuracy: 0.4190\n",
            "Epoch 193/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.4771 - val_accuracy: 0.4182\n",
            "Epoch 194/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.4755 - val_accuracy: 0.4187\n",
            "Epoch 195/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 8.9764e-04 - accuracy: 0.9999 - val_loss: 11.4905 - val_accuracy: 0.4182\n",
            "Epoch 196/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.4953 - val_accuracy: 0.4171\n",
            "Epoch 197/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 11.4955 - val_accuracy: 0.4179\n",
            "Epoch 198/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.4999 - val_accuracy: 0.4185\n",
            "Epoch 199/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.4954 - val_accuracy: 0.4185\n",
            "Epoch 200/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 11.5019 - val_accuracy: 0.4185\n",
            "Epoch 201/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 11.5143 - val_accuracy: 0.4185\n",
            "Epoch 202/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5015 - val_accuracy: 0.4182\n",
            "Epoch 203/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 11.5092 - val_accuracy: 0.4185\n",
            "Epoch 204/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.5120 - val_accuracy: 0.4187\n",
            "Epoch 205/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5173 - val_accuracy: 0.4190\n",
            "Epoch 206/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 11.5159 - val_accuracy: 0.4182\n",
            "Epoch 207/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5278 - val_accuracy: 0.4185\n",
            "Epoch 208/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.5328 - val_accuracy: 0.4190\n",
            "Epoch 209/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 5.5093e-04 - accuracy: 0.9999 - val_loss: 11.5337 - val_accuracy: 0.4187\n",
            "Epoch 210/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.5370 - val_accuracy: 0.4201\n",
            "Epoch 211/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.5576 - val_accuracy: 0.4201\n",
            "Epoch 212/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.5438 - val_accuracy: 0.4203\n",
            "Epoch 213/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.5586 - val_accuracy: 0.4193\n",
            "Epoch 214/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.5587 - val_accuracy: 0.4195\n",
            "Epoch 215/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.5556 - val_accuracy: 0.4203\n",
            "Epoch 216/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 11.5762 - val_accuracy: 0.4195\n",
            "Epoch 217/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 9.1745e-04 - accuracy: 0.9999 - val_loss: 11.5686 - val_accuracy: 0.4190\n",
            "Epoch 218/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 11.5801 - val_accuracy: 0.4201\n",
            "Epoch 219/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.5727 - val_accuracy: 0.4212\n",
            "Epoch 220/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 11.5878 - val_accuracy: 0.4193\n",
            "Epoch 221/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.5922 - val_accuracy: 0.4201\n",
            "Epoch 222/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.5858 - val_accuracy: 0.4222\n",
            "Epoch 223/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6008 - val_accuracy: 0.4220\n",
            "Epoch 224/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6046 - val_accuracy: 0.4220\n",
            "Epoch 225/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 6.7985e-04 - accuracy: 0.9999 - val_loss: 11.6115 - val_accuracy: 0.4225\n",
            "Epoch 226/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 11.6134 - val_accuracy: 0.4222\n",
            "Epoch 227/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 9.8383e-04 - accuracy: 0.9998 - val_loss: 11.6136 - val_accuracy: 0.4217\n",
            "Epoch 228/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6166 - val_accuracy: 0.4217\n",
            "Epoch 229/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6067 - val_accuracy: 0.4225\n",
            "Epoch 230/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.6315 - val_accuracy: 0.4214\n",
            "Epoch 231/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.6358 - val_accuracy: 0.4217\n",
            "Epoch 232/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.6283 - val_accuracy: 0.4220\n",
            "Epoch 233/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6409 - val_accuracy: 0.4222\n",
            "Epoch 234/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 11.6419 - val_accuracy: 0.4212\n",
            "Epoch 235/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.5258e-04 - accuracy: 0.9998 - val_loss: 11.6452 - val_accuracy: 0.4220\n",
            "Epoch 236/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6478 - val_accuracy: 0.4203\n",
            "Epoch 237/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6565 - val_accuracy: 0.4198\n",
            "Epoch 238/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.6551 - val_accuracy: 0.4217\n",
            "Epoch 239/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.6586 - val_accuracy: 0.4214\n",
            "Epoch 240/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6668 - val_accuracy: 0.4206\n",
            "Epoch 241/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6671 - val_accuracy: 0.4217\n",
            "Epoch 242/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 6.5420e-04 - accuracy: 0.9999 - val_loss: 11.6647 - val_accuracy: 0.4228\n",
            "Epoch 243/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6670 - val_accuracy: 0.4233\n",
            "Epoch 244/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.6836 - val_accuracy: 0.4217\n",
            "Epoch 245/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6723 - val_accuracy: 0.4233\n",
            "Epoch 246/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6818 - val_accuracy: 0.4238\n",
            "Epoch 247/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6798 - val_accuracy: 0.4228\n",
            "Epoch 248/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 7.1944e-04 - accuracy: 0.9999 - val_loss: 11.6742 - val_accuracy: 0.4236\n",
            "Epoch 249/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.6707 - val_accuracy: 0.4236\n",
            "Epoch 250/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6936 - val_accuracy: 0.4228\n",
            "Epoch 251/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6872 - val_accuracy: 0.4244\n",
            "Epoch 252/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6915 - val_accuracy: 0.4236\n",
            "Epoch 253/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6963 - val_accuracy: 0.4244\n",
            "Epoch 254/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6907 - val_accuracy: 0.4247\n",
            "Epoch 255/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 9.8080e-04 - accuracy: 0.9998 - val_loss: 11.6868 - val_accuracy: 0.4252\n",
            "Epoch 256/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.6309e-04 - accuracy: 0.9999 - val_loss: 11.6752 - val_accuracy: 0.4257\n",
            "Epoch 257/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.6883 - val_accuracy: 0.4252\n",
            "Epoch 258/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6811 - val_accuracy: 0.4255\n",
            "Epoch 259/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6693 - val_accuracy: 0.4249\n",
            "Epoch 260/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.4785e-04 - accuracy: 0.9999 - val_loss: 11.6872 - val_accuracy: 0.4255\n",
            "Epoch 261/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6804 - val_accuracy: 0.4244\n",
            "Epoch 262/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.6659 - val_accuracy: 0.4252\n",
            "Epoch 263/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6636 - val_accuracy: 0.4276\n",
            "Epoch 264/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6733 - val_accuracy: 0.4263\n",
            "Epoch 265/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6576 - val_accuracy: 0.4284\n",
            "Epoch 266/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6579 - val_accuracy: 0.4279\n",
            "Epoch 267/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6571 - val_accuracy: 0.4276\n",
            "Epoch 268/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6603 - val_accuracy: 0.4276\n",
            "Epoch 269/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 11.6483 - val_accuracy: 0.4271\n",
            "Epoch 270/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6452 - val_accuracy: 0.4281\n",
            "Epoch 271/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.6359 - val_accuracy: 0.4279\n",
            "Epoch 272/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.6312 - val_accuracy: 0.4279\n",
            "Epoch 273/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6199 - val_accuracy: 0.4273\n",
            "Epoch 274/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 7.5070e-04 - accuracy: 0.9999 - val_loss: 11.6063 - val_accuracy: 0.4287\n",
            "Epoch 275/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6039 - val_accuracy: 0.4284\n",
            "Epoch 276/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6041 - val_accuracy: 0.4295\n",
            "Epoch 277/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.5833 - val_accuracy: 0.4306\n",
            "Epoch 278/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.5799 - val_accuracy: 0.4314\n",
            "Epoch 279/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.2398e-04 - accuracy: 0.9999 - val_loss: 11.5663 - val_accuracy: 0.4306\n",
            "Epoch 280/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.5555 - val_accuracy: 0.4311\n",
            "Epoch 281/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.5575 - val_accuracy: 0.4319\n",
            "Epoch 282/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 11.5467 - val_accuracy: 0.4298\n",
            "Epoch 283/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.5291 - val_accuracy: 0.4316\n",
            "Epoch 284/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5121 - val_accuracy: 0.4333\n",
            "Epoch 285/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5084 - val_accuracy: 0.4330\n",
            "Epoch 286/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5163 - val_accuracy: 0.4330\n",
            "Epoch 287/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.5003 - val_accuracy: 0.4306\n",
            "Epoch 288/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.4998 - val_accuracy: 0.4325\n",
            "Epoch 289/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.4881 - val_accuracy: 0.4322\n",
            "Epoch 290/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.4766 - val_accuracy: 0.4316\n",
            "Epoch 291/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 11.4851 - val_accuracy: 0.4292\n",
            "Epoch 292/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.4528 - val_accuracy: 0.4306\n",
            "Epoch 293/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.4016e-04 - accuracy: 0.9999 - val_loss: 11.4512 - val_accuracy: 0.4319\n",
            "Epoch 294/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.4437 - val_accuracy: 0.4308\n",
            "Epoch 295/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.4524 - val_accuracy: 0.4319\n",
            "Epoch 296/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 6.3432e-04 - accuracy: 0.9999 - val_loss: 11.4342 - val_accuracy: 0.4333\n",
            "Epoch 297/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.4235 - val_accuracy: 0.4316\n",
            "Epoch 298/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 11.4197 - val_accuracy: 0.4311\n",
            "Epoch 299/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.4108 - val_accuracy: 0.4306\n",
            "Epoch 300/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.4186 - val_accuracy: 0.4300\n",
            "Epoch 301/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.4092 - val_accuracy: 0.4333\n",
            "Epoch 302/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 8.2976e-04 - accuracy: 0.9999 - val_loss: 11.3924 - val_accuracy: 0.4311\n",
            "Epoch 303/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 11.3987 - val_accuracy: 0.4322\n",
            "Epoch 304/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 11.3841 - val_accuracy: 0.4306\n",
            "Epoch 305/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 11.3985 - val_accuracy: 0.4300\n",
            "Epoch 306/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 12.9256 - val_accuracy: 0.3940\n",
            "Epoch 307/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0317 - accuracy: 0.9925 - val_loss: 12.6740 - val_accuracy: 0.4096\n",
            "Epoch 308/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 12.3502 - val_accuracy: 0.4150\n",
            "Epoch 309/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 12.2592 - val_accuracy: 0.4217\n",
            "Epoch 310/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.2097 - val_accuracy: 0.4214\n",
            "Epoch 311/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.2029 - val_accuracy: 0.4236\n",
            "Epoch 312/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1971 - val_accuracy: 0.4236\n",
            "Epoch 313/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1928 - val_accuracy: 0.4238\n",
            "Epoch 314/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1883 - val_accuracy: 0.4236\n",
            "Epoch 315/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1874 - val_accuracy: 0.4233\n",
            "Epoch 316/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1807 - val_accuracy: 0.4236\n",
            "Epoch 317/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1792 - val_accuracy: 0.4238\n",
            "Epoch 318/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 8.1454e-04 - accuracy: 0.9999 - val_loss: 12.1796 - val_accuracy: 0.4238\n",
            "Epoch 319/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1727 - val_accuracy: 0.4241\n",
            "Epoch 320/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1735 - val_accuracy: 0.4244\n",
            "Epoch 321/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1726 - val_accuracy: 0.4255\n",
            "Epoch 322/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 8.3853e-04 - accuracy: 0.9999 - val_loss: 12.1718 - val_accuracy: 0.4252\n",
            "Epoch 323/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1662 - val_accuracy: 0.4255\n",
            "Epoch 324/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1744 - val_accuracy: 0.4263\n",
            "Epoch 325/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1664 - val_accuracy: 0.4255\n",
            "Epoch 326/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1680 - val_accuracy: 0.4260\n",
            "Epoch 327/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1706 - val_accuracy: 0.4265\n",
            "Epoch 328/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1645 - val_accuracy: 0.4265\n",
            "Epoch 329/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.1609 - val_accuracy: 0.4263\n",
            "Epoch 330/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.1674 - val_accuracy: 0.4265\n",
            "Epoch 331/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 8.9110e-04 - accuracy: 0.9999 - val_loss: 12.1628 - val_accuracy: 0.4268\n",
            "Epoch 332/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1630 - val_accuracy: 0.4271\n",
            "Epoch 333/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1691 - val_accuracy: 0.4271\n",
            "Epoch 334/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 9.5685e-04 - accuracy: 0.9999 - val_loss: 12.1586 - val_accuracy: 0.4265\n",
            "Epoch 335/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.1611 - val_accuracy: 0.4265\n",
            "Epoch 336/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1582 - val_accuracy: 0.4265\n",
            "Epoch 337/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1610 - val_accuracy: 0.4263\n",
            "Epoch 338/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1587 - val_accuracy: 0.4265\n",
            "Epoch 339/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1667 - val_accuracy: 0.4268\n",
            "Epoch 340/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1610 - val_accuracy: 0.4263\n",
            "Epoch 341/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1632 - val_accuracy: 0.4265\n",
            "Epoch 342/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 12.1607 - val_accuracy: 0.4271\n",
            "Epoch 343/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 7.9980e-04 - accuracy: 0.9999 - val_loss: 12.1717 - val_accuracy: 0.4276\n",
            "Epoch 344/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1637 - val_accuracy: 0.4276\n",
            "Epoch 345/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.1626 - val_accuracy: 0.4268\n",
            "Epoch 346/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1635 - val_accuracy: 0.4268\n",
            "Epoch 347/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.3262e-04 - accuracy: 0.9999 - val_loss: 12.1723 - val_accuracy: 0.4276\n",
            "Epoch 348/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1687 - val_accuracy: 0.4271\n",
            "Epoch 349/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1723 - val_accuracy: 0.4276\n",
            "Epoch 350/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1630 - val_accuracy: 0.4276\n",
            "Epoch 351/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.1633 - val_accuracy: 0.4287\n",
            "Epoch 352/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1732 - val_accuracy: 0.4290\n",
            "Epoch 353/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.1689 - val_accuracy: 0.4290\n",
            "Epoch 354/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.1797 - val_accuracy: 0.4298\n",
            "Epoch 355/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1657 - val_accuracy: 0.4295\n",
            "Epoch 356/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 7.7915e-04 - accuracy: 0.9999 - val_loss: 12.1825 - val_accuracy: 0.4298\n",
            "Epoch 357/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 12.1699 - val_accuracy: 0.4298\n",
            "Epoch 358/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1675 - val_accuracy: 0.4295\n",
            "Epoch 359/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 8.1293e-04 - accuracy: 0.9999 - val_loss: 12.1866 - val_accuracy: 0.4306\n",
            "Epoch 360/1000\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1715 - val_accuracy: 0.4295\n",
            "Epoch 361/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1818 - val_accuracy: 0.4298\n",
            "Epoch 362/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 9.5336e-04 - accuracy: 0.9999 - val_loss: 12.1750 - val_accuracy: 0.4295\n",
            "Epoch 363/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1708 - val_accuracy: 0.4298\n",
            "Epoch 364/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1718 - val_accuracy: 0.4300\n",
            "Epoch 365/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1758 - val_accuracy: 0.4300\n",
            "Epoch 366/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.8245e-04 - accuracy: 0.9998 - val_loss: 12.1802 - val_accuracy: 0.4303\n",
            "Epoch 367/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 9.0622e-04 - accuracy: 0.9999 - val_loss: 12.1783 - val_accuracy: 0.4308\n",
            "Epoch 368/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1749 - val_accuracy: 0.4306\n",
            "Epoch 369/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1781 - val_accuracy: 0.4311\n",
            "Epoch 370/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1908 - val_accuracy: 0.4314\n",
            "Epoch 371/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1767 - val_accuracy: 0.4316\n",
            "Epoch 372/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1872 - val_accuracy: 0.4311\n",
            "Epoch 373/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1878 - val_accuracy: 0.4308\n",
            "Epoch 374/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.6793e-04 - accuracy: 0.9999 - val_loss: 12.1821 - val_accuracy: 0.4303\n",
            "Epoch 375/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 12.1893 - val_accuracy: 0.4306\n",
            "Epoch 376/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1792 - val_accuracy: 0.4303\n",
            "Epoch 377/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1861 - val_accuracy: 0.4306\n",
            "Epoch 378/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.9877e-04 - accuracy: 0.9999 - val_loss: 12.1831 - val_accuracy: 0.4303\n",
            "Epoch 379/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1906 - val_accuracy: 0.4306\n",
            "Epoch 380/1000\n",
            "157/157 [==============================] - 10s 67ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1800 - val_accuracy: 0.4306\n",
            "Epoch 381/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1796 - val_accuracy: 0.4303\n",
            "Epoch 382/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1903 - val_accuracy: 0.4311\n",
            "Epoch 383/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 12.1879 - val_accuracy: 0.4308\n",
            "Epoch 384/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.5556e-04 - accuracy: 0.9999 - val_loss: 12.1782 - val_accuracy: 0.4308\n",
            "Epoch 385/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1797 - val_accuracy: 0.4308\n",
            "Epoch 386/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.1784 - val_accuracy: 0.4316\n",
            "Epoch 387/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1872 - val_accuracy: 0.4314\n",
            "Epoch 388/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1781 - val_accuracy: 0.4308\n",
            "Epoch 389/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1763 - val_accuracy: 0.4311\n",
            "Epoch 390/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1699 - val_accuracy: 0.4308\n",
            "Epoch 391/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1717 - val_accuracy: 0.4314\n",
            "Epoch 392/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1834 - val_accuracy: 0.4316\n",
            "Epoch 393/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.1691 - val_accuracy: 0.4314\n",
            "Epoch 394/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1642 - val_accuracy: 0.4311\n",
            "Epoch 395/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.1758 - val_accuracy: 0.4314\n",
            "Epoch 396/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1624 - val_accuracy: 0.4308\n",
            "Epoch 397/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1748 - val_accuracy: 0.4314\n",
            "Epoch 398/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1678 - val_accuracy: 0.4314\n",
            "Epoch 399/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 12.1515 - val_accuracy: 0.4308\n",
            "Epoch 400/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1560 - val_accuracy: 0.4306\n",
            "Epoch 401/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.7875e-04 - accuracy: 0.9999 - val_loss: 12.1480 - val_accuracy: 0.4308\n",
            "Epoch 402/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.1446 - val_accuracy: 0.4303\n",
            "Epoch 403/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.1447 - val_accuracy: 0.4303\n",
            "Epoch 404/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.1460 - val_accuracy: 0.4303\n",
            "Epoch 405/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.6549e-04 - accuracy: 0.9999 - val_loss: 12.1261 - val_accuracy: 0.4300\n",
            "Epoch 406/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 12.1402 - val_accuracy: 0.4314\n",
            "Epoch 407/1000\n",
            "157/157 [==============================] - 10s 67ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1270 - val_accuracy: 0.4314\n",
            "Epoch 408/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.1137 - val_accuracy: 0.4319\n",
            "Epoch 409/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1195 - val_accuracy: 0.4319\n",
            "Epoch 410/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 5.4921e-04 - accuracy: 0.9999 - val_loss: 12.1035 - val_accuracy: 0.4319\n",
            "Epoch 411/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.0919 - val_accuracy: 0.4327\n",
            "Epoch 412/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.0952 - val_accuracy: 0.4314\n",
            "Epoch 413/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.6723e-04 - accuracy: 0.9999 - val_loss: 12.0842 - val_accuracy: 0.4330\n",
            "Epoch 414/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 12.0870 - val_accuracy: 0.4333\n",
            "Epoch 415/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.0764 - val_accuracy: 0.4325\n",
            "Epoch 416/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.0671 - val_accuracy: 0.4330\n",
            "Epoch 417/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.0623 - val_accuracy: 0.4333\n",
            "Epoch 418/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.0540 - val_accuracy: 0.4343\n",
            "Epoch 419/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.0371 - val_accuracy: 0.4341\n",
            "Epoch 420/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.0320 - val_accuracy: 0.4338\n",
            "Epoch 421/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.0305 - val_accuracy: 0.4335\n",
            "Epoch 422/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.0218 - val_accuracy: 0.4341\n",
            "Epoch 423/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.0086 - val_accuracy: 0.4338\n",
            "Epoch 424/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.9969 - val_accuracy: 0.4333\n",
            "Epoch 425/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.9839 - val_accuracy: 0.4338\n",
            "Epoch 426/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.9855 - val_accuracy: 0.4330\n",
            "Epoch 427/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 8.9801e-04 - accuracy: 0.9999 - val_loss: 11.9622 - val_accuracy: 0.4338\n",
            "Epoch 428/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.9612 - val_accuracy: 0.4330\n",
            "Epoch 429/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.9445 - val_accuracy: 0.4338\n",
            "Epoch 430/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.9541 - val_accuracy: 0.4335\n",
            "Epoch 431/1000\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.9314 - val_accuracy: 0.4341\n",
            "Epoch 432/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.9284 - val_accuracy: 0.4335\n",
            "Epoch 433/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.9260 - val_accuracy: 0.4325\n",
            "Epoch 434/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.9045 - val_accuracy: 0.4346\n",
            "Epoch 435/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.8972 - val_accuracy: 0.4343\n",
            "Epoch 436/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.8955 - val_accuracy: 0.4343\n",
            "Epoch 437/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.8724 - val_accuracy: 0.4343\n",
            "Epoch 438/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.8801 - val_accuracy: 0.4354\n",
            "Epoch 439/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8611 - val_accuracy: 0.4346\n",
            "Epoch 440/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.8522 - val_accuracy: 0.4333\n",
            "Epoch 441/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8389 - val_accuracy: 0.4335\n",
            "Epoch 442/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.8311 - val_accuracy: 0.4333\n",
            "Epoch 443/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.8437 - val_accuracy: 0.4335\n",
            "Epoch 444/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.2090e-04 - accuracy: 0.9999 - val_loss: 11.8142 - val_accuracy: 0.4327\n",
            "Epoch 445/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.8145 - val_accuracy: 0.4338\n",
            "Epoch 446/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.8166 - val_accuracy: 0.4338\n",
            "Epoch 447/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.7967 - val_accuracy: 0.4341\n",
            "Epoch 448/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 11.7885 - val_accuracy: 0.4338\n",
            "Epoch 449/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.7845 - val_accuracy: 0.4338\n",
            "Epoch 450/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.7943 - val_accuracy: 0.4335\n",
            "Epoch 451/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.7735 - val_accuracy: 0.4360\n",
            "Epoch 452/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.7885e-04 - accuracy: 0.9999 - val_loss: 11.7568 - val_accuracy: 0.4349\n",
            "Epoch 453/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.7546 - val_accuracy: 0.4341\n",
            "Epoch 454/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 11.7469 - val_accuracy: 0.4360\n",
            "Epoch 455/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.7660 - val_accuracy: 0.4346\n",
            "Epoch 456/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.3253e-04 - accuracy: 0.9999 - val_loss: 11.7385 - val_accuracy: 0.4351\n",
            "Epoch 457/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.7350 - val_accuracy: 0.4349\n",
            "Epoch 458/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.4643e-04 - accuracy: 0.9999 - val_loss: 11.7268 - val_accuracy: 0.4349\n",
            "Epoch 459/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.7289 - val_accuracy: 0.4335\n",
            "Epoch 460/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.7193 - val_accuracy: 0.4341\n",
            "Epoch 461/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.7061 - val_accuracy: 0.4333\n",
            "Epoch 462/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.7101 - val_accuracy: 0.4341\n",
            "Epoch 463/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.7186 - val_accuracy: 0.4325\n",
            "Epoch 464/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.7036 - val_accuracy: 0.4338\n",
            "Epoch 465/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6964 - val_accuracy: 0.4322\n",
            "Epoch 466/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.7004 - val_accuracy: 0.4333\n",
            "Epoch 467/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.7033 - val_accuracy: 0.4316\n",
            "Epoch 468/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6781 - val_accuracy: 0.4308\n",
            "Epoch 469/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6823 - val_accuracy: 0.4322\n",
            "Epoch 470/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6847 - val_accuracy: 0.4327\n",
            "Epoch 471/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6718 - val_accuracy: 0.4303\n",
            "Epoch 472/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6768 - val_accuracy: 0.4322\n",
            "Epoch 473/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6712 - val_accuracy: 0.4322\n",
            "Epoch 474/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6768 - val_accuracy: 0.4314\n",
            "Epoch 475/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6593 - val_accuracy: 0.4308\n",
            "Epoch 476/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6594 - val_accuracy: 0.4303\n",
            "Epoch 477/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.6602 - val_accuracy: 0.4308\n",
            "Epoch 478/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.6712 - val_accuracy: 0.4316\n",
            "Epoch 479/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 11.6559 - val_accuracy: 0.4311\n",
            "Epoch 480/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.6516 - val_accuracy: 0.4311\n",
            "Epoch 481/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6439 - val_accuracy: 0.4298\n",
            "Epoch 482/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6565 - val_accuracy: 0.4314\n",
            "Epoch 483/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 11.6435 - val_accuracy: 0.4306\n",
            "Epoch 484/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6373 - val_accuracy: 0.4300\n",
            "Epoch 485/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.6423 - val_accuracy: 0.4300\n",
            "Epoch 486/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6397 - val_accuracy: 0.4300\n",
            "Epoch 487/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6424 - val_accuracy: 0.4306\n",
            "Epoch 488/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.9388e-04 - accuracy: 0.9999 - val_loss: 11.6342 - val_accuracy: 0.4300\n",
            "Epoch 489/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.6284 - val_accuracy: 0.4308\n",
            "Epoch 490/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6470 - val_accuracy: 0.4306\n",
            "Epoch 491/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6319 - val_accuracy: 0.4316\n",
            "Epoch 492/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6324 - val_accuracy: 0.4295\n",
            "Epoch 493/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.6307 - val_accuracy: 0.4311\n",
            "Epoch 494/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6193 - val_accuracy: 0.4306\n",
            "Epoch 495/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6127 - val_accuracy: 0.4295\n",
            "Epoch 496/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.6200 - val_accuracy: 0.4292\n",
            "Epoch 497/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.6128 - val_accuracy: 0.4279\n",
            "Epoch 498/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 11.6039 - val_accuracy: 0.4295\n",
            "Epoch 499/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.1809e-04 - accuracy: 0.9999 - val_loss: 11.6110 - val_accuracy: 0.4268\n",
            "Epoch 500/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.6268 - val_accuracy: 0.4284\n",
            "Epoch 501/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6044 - val_accuracy: 0.4273\n",
            "Epoch 502/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.5976 - val_accuracy: 0.4273\n",
            "Epoch 503/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 11.6060 - val_accuracy: 0.4300\n",
            "Epoch 504/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 11.6053 - val_accuracy: 0.4298\n",
            "Epoch 505/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.6034 - val_accuracy: 0.4265\n",
            "Epoch 506/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5949 - val_accuracy: 0.4271\n",
            "Epoch 507/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5957 - val_accuracy: 0.4292\n",
            "Epoch 508/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.6009 - val_accuracy: 0.4284\n",
            "Epoch 509/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5941 - val_accuracy: 0.4268\n",
            "Epoch 510/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 11.5894 - val_accuracy: 0.4292\n",
            "Epoch 511/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.5807 - val_accuracy: 0.4265\n",
            "Epoch 512/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.5790 - val_accuracy: 0.4276\n",
            "Epoch 513/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.9270e-04 - accuracy: 0.9999 - val_loss: 11.5861 - val_accuracy: 0.4290\n",
            "Epoch 514/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5899 - val_accuracy: 0.4279\n",
            "Epoch 515/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5972 - val_accuracy: 0.4287\n",
            "Epoch 516/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5699 - val_accuracy: 0.4268\n",
            "Epoch 517/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 11.5763 - val_accuracy: 0.4281\n",
            "Epoch 518/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.5906e-04 - accuracy: 0.9999 - val_loss: 11.5891 - val_accuracy: 0.4276\n",
            "Epoch 519/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5736 - val_accuracy: 0.4279\n",
            "Epoch 520/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.5752 - val_accuracy: 0.4284\n",
            "Epoch 521/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.1058e-04 - accuracy: 0.9999 - val_loss: 11.5689 - val_accuracy: 0.4276\n",
            "Epoch 522/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.5701 - val_accuracy: 0.4281\n",
            "Epoch 523/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 11.5659 - val_accuracy: 0.4268\n",
            "Epoch 524/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.6702e-04 - accuracy: 0.9999 - val_loss: 11.5866 - val_accuracy: 0.4252\n",
            "Epoch 525/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.5608 - val_accuracy: 0.4276\n",
            "Epoch 526/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.5671 - val_accuracy: 0.4276\n",
            "Epoch 527/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.1685e-04 - accuracy: 0.9999 - val_loss: 11.5810 - val_accuracy: 0.4265\n",
            "Epoch 528/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.5569 - val_accuracy: 0.4271\n",
            "Epoch 529/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 11.5716 - val_accuracy: 0.4268\n",
            "Epoch 530/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.5563 - val_accuracy: 0.4268\n",
            "Epoch 531/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 11.5541 - val_accuracy: 0.4273\n",
            "Epoch 532/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5676 - val_accuracy: 0.4287\n",
            "Epoch 533/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.5545 - val_accuracy: 0.4279\n",
            "Epoch 534/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 8.4657e-04 - accuracy: 0.9999 - val_loss: 11.5596 - val_accuracy: 0.4271\n",
            "Epoch 535/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5469 - val_accuracy: 0.4271\n",
            "Epoch 536/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.5775 - val_accuracy: 0.4265\n",
            "Epoch 537/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 11.5572 - val_accuracy: 0.4265\n",
            "Epoch 538/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.5596 - val_accuracy: 0.4265\n",
            "Epoch 539/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5604 - val_accuracy: 0.4252\n",
            "Epoch 540/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.5463 - val_accuracy: 0.4265\n",
            "Epoch 541/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5656 - val_accuracy: 0.4263\n",
            "Epoch 542/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.5617 - val_accuracy: 0.4263\n",
            "Epoch 543/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.5278e-04 - accuracy: 0.9999 - val_loss: 11.5486 - val_accuracy: 0.4244\n",
            "Epoch 544/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5373 - val_accuracy: 0.4260\n",
            "Epoch 545/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.5455 - val_accuracy: 0.4255\n",
            "Epoch 546/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5576 - val_accuracy: 0.4271\n",
            "Epoch 547/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5402 - val_accuracy: 0.4279\n",
            "Epoch 548/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.5400 - val_accuracy: 0.4263\n",
            "Epoch 549/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5531 - val_accuracy: 0.4257\n",
            "Epoch 550/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.3799e-04 - accuracy: 0.9999 - val_loss: 11.5439 - val_accuracy: 0.4249\n",
            "Epoch 551/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 11.5318 - val_accuracy: 0.4271\n",
            "Epoch 552/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.5392 - val_accuracy: 0.4263\n",
            "Epoch 553/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.5345 - val_accuracy: 0.4263\n",
            "Epoch 554/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.5352 - val_accuracy: 0.4271\n",
            "Epoch 555/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.5535 - val_accuracy: 0.4265\n",
            "Epoch 556/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.5333 - val_accuracy: 0.4268\n",
            "Epoch 557/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.5393 - val_accuracy: 0.4260\n",
            "Epoch 558/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 11.5743 - val_accuracy: 0.4249\n",
            "Epoch 559/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 13.1131 - val_accuracy: 0.3991\n",
            "Epoch 560/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0223 - accuracy: 0.9945 - val_loss: 12.6905 - val_accuracy: 0.4150\n",
            "Epoch 561/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 12.5219 - val_accuracy: 0.4163\n",
            "Epoch 562/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 12.4198 - val_accuracy: 0.4198\n",
            "Epoch 563/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.3743e-04 - accuracy: 0.9999 - val_loss: 12.3515 - val_accuracy: 0.4206\n",
            "Epoch 564/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.3370 - val_accuracy: 0.4217\n",
            "Epoch 565/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 12.3308 - val_accuracy: 0.4217\n",
            "Epoch 566/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 6.2851e-04 - accuracy: 0.9999 - val_loss: 12.3244 - val_accuracy: 0.4225\n",
            "Epoch 567/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.3102 - val_accuracy: 0.4230\n",
            "Epoch 568/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.6718e-04 - accuracy: 0.9999 - val_loss: 12.3125 - val_accuracy: 0.4236\n",
            "Epoch 569/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.3087 - val_accuracy: 0.4241\n",
            "Epoch 570/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2994 - val_accuracy: 0.4247\n",
            "Epoch 571/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 6.7826e-04 - accuracy: 0.9999 - val_loss: 12.2913 - val_accuracy: 0.4244\n",
            "Epoch 572/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 12.2967 - val_accuracy: 0.4263\n",
            "Epoch 573/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.2935 - val_accuracy: 0.4257\n",
            "Epoch 574/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.2785 - val_accuracy: 0.4252\n",
            "Epoch 575/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.2787 - val_accuracy: 0.4257\n",
            "Epoch 576/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 12.2862 - val_accuracy: 0.4263\n",
            "Epoch 577/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.2812 - val_accuracy: 0.4263\n",
            "Epoch 578/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.2651 - val_accuracy: 0.4257\n",
            "Epoch 579/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.2765 - val_accuracy: 0.4268\n",
            "Epoch 580/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 7.3671e-04 - accuracy: 0.9999 - val_loss: 12.2638 - val_accuracy: 0.4263\n",
            "Epoch 581/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.2603 - val_accuracy: 0.4265\n",
            "Epoch 582/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.2593 - val_accuracy: 0.4273\n",
            "Epoch 583/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 12.2605 - val_accuracy: 0.4273\n",
            "Epoch 584/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 12.2500 - val_accuracy: 0.4271\n",
            "Epoch 585/1000\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.2585 - val_accuracy: 0.4271\n",
            "Epoch 586/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.1809e-04 - accuracy: 0.9999 - val_loss: 12.2472 - val_accuracy: 0.4260\n",
            "Epoch 587/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.2444 - val_accuracy: 0.4268\n",
            "Epoch 588/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.2420 - val_accuracy: 0.4273\n",
            "Epoch 589/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.2395 - val_accuracy: 0.4273\n",
            "Epoch 590/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.2367 - val_accuracy: 0.4281\n",
            "Epoch 591/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.2382 - val_accuracy: 0.4284\n",
            "Epoch 592/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.2351 - val_accuracy: 0.4287\n",
            "Epoch 593/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 12.2450 - val_accuracy: 0.4287\n",
            "Epoch 594/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.2523 - val_accuracy: 0.4290\n",
            "Epoch 595/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.3076e-04 - accuracy: 0.9999 - val_loss: 12.2341 - val_accuracy: 0.4284\n",
            "Epoch 596/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.2330 - val_accuracy: 0.4284\n",
            "Epoch 597/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 12.2495 - val_accuracy: 0.4290\n",
            "Epoch 598/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2469 - val_accuracy: 0.4292\n",
            "Epoch 599/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 12.2319 - val_accuracy: 0.4292\n",
            "Epoch 600/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 6.8625e-04 - accuracy: 0.9999 - val_loss: 12.2293 - val_accuracy: 0.4290\n",
            "Epoch 601/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 12.2262 - val_accuracy: 0.4284\n",
            "Epoch 602/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 12.2261 - val_accuracy: 0.4284\n",
            "Epoch 603/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2253 - val_accuracy: 0.4284\n",
            "Epoch 604/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.2536 - val_accuracy: 0.4295\n",
            "Epoch 605/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.2203 - val_accuracy: 0.4284\n",
            "Epoch 606/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.2215 - val_accuracy: 0.4281\n",
            "Epoch 607/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.2217 - val_accuracy: 0.4287\n",
            "Epoch 608/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.2291 - val_accuracy: 0.4290\n",
            "Epoch 609/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 12.2180 - val_accuracy: 0.4284\n",
            "Epoch 610/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 7.2019e-04 - accuracy: 0.9999 - val_loss: 12.2410 - val_accuracy: 0.4295\n",
            "Epoch 611/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.2202 - val_accuracy: 0.4292\n",
            "Epoch 612/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 12.2158 - val_accuracy: 0.4290\n",
            "Epoch 613/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.2152 - val_accuracy: 0.4284\n",
            "Epoch 614/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2136 - val_accuracy: 0.4284\n",
            "Epoch 615/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.2127 - val_accuracy: 0.4287\n",
            "Epoch 616/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2115 - val_accuracy: 0.4290\n",
            "Epoch 617/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 12.2186 - val_accuracy: 0.4295\n",
            "Epoch 618/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.2171 - val_accuracy: 0.4292\n",
            "Epoch 619/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 12.2098 - val_accuracy: 0.4281\n",
            "Epoch 620/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.2092 - val_accuracy: 0.4284\n",
            "Epoch 621/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.2075 - val_accuracy: 0.4284\n",
            "Epoch 622/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.2257 - val_accuracy: 0.4295\n",
            "Epoch 623/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.2180 - val_accuracy: 0.4303\n",
            "Epoch 624/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.4200e-04 - accuracy: 0.9999 - val_loss: 12.2045 - val_accuracy: 0.4295\n",
            "Epoch 625/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.2168 - val_accuracy: 0.4300\n",
            "Epoch 626/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.2181 - val_accuracy: 0.4300\n",
            "Epoch 627/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.6735e-04 - accuracy: 0.9999 - val_loss: 12.2018 - val_accuracy: 0.4292\n",
            "Epoch 628/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 12.2009 - val_accuracy: 0.4292\n",
            "Epoch 629/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.2612e-04 - accuracy: 0.9999 - val_loss: 12.1993 - val_accuracy: 0.4292\n",
            "Epoch 630/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.2199 - val_accuracy: 0.4308\n",
            "Epoch 631/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 12.2160 - val_accuracy: 0.4314\n",
            "Epoch 632/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 12.1945 - val_accuracy: 0.4298\n",
            "Epoch 633/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 12.2068 - val_accuracy: 0.4308\n",
            "Epoch 634/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.1926 - val_accuracy: 0.4300\n",
            "Epoch 635/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 12.2114 - val_accuracy: 0.4314\n",
            "Epoch 636/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 6.9072e-04 - accuracy: 0.9998 - val_loss: 12.1930 - val_accuracy: 0.4311\n",
            "Epoch 637/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.2170 - val_accuracy: 0.4311\n",
            "Epoch 638/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 12.2135 - val_accuracy: 0.4311\n",
            "Epoch 639/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.2100 - val_accuracy: 0.4314\n",
            "Epoch 640/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.2039 - val_accuracy: 0.4322\n",
            "Epoch 641/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 12.1922 - val_accuracy: 0.4322\n",
            "Epoch 642/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 12.1782 - val_accuracy: 0.4316\n",
            "Epoch 643/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 7.3928e-04 - accuracy: 0.9999 - val_loss: 12.1940 - val_accuracy: 0.4325\n",
            "Epoch 644/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 9.0457e-04 - accuracy: 0.9999 - val_loss: 12.1748 - val_accuracy: 0.4311\n",
            "Epoch 645/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1781 - val_accuracy: 0.4325\n",
            "Epoch 646/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 12.1896 - val_accuracy: 0.4319\n",
            "Epoch 647/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 12.1826 - val_accuracy: 0.4325\n",
            "Epoch 648/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.1638 - val_accuracy: 0.4314\n",
            "Epoch 649/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 12.1613 - val_accuracy: 0.4314\n",
            "Epoch 650/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1599 - val_accuracy: 0.4319\n",
            "Epoch 651/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.1549 - val_accuracy: 0.4319\n",
            "Epoch 652/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1710 - val_accuracy: 0.4327\n",
            "Epoch 653/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 7.8618e-04 - accuracy: 0.9999 - val_loss: 12.1500 - val_accuracy: 0.4319\n",
            "Epoch 654/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1453 - val_accuracy: 0.4333\n",
            "Epoch 655/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.1467 - val_accuracy: 0.4333\n",
            "Epoch 656/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 12.1589 - val_accuracy: 0.4335\n",
            "Epoch 657/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 8.9530e-04 - accuracy: 0.9998 - val_loss: 12.1327 - val_accuracy: 0.4333\n",
            "Epoch 658/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 12.1369 - val_accuracy: 0.4333\n",
            "Epoch 659/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 12.1501 - val_accuracy: 0.4341\n",
            "Epoch 660/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 12.1309 - val_accuracy: 0.4357\n",
            "Epoch 661/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1305 - val_accuracy: 0.4357\n",
            "Epoch 662/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 12.1289 - val_accuracy: 0.4351\n",
            "Epoch 663/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 12.1086 - val_accuracy: 0.4341\n",
            "Epoch 664/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 12.1250 - val_accuracy: 0.4351\n",
            "Epoch 665/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 12.1129 - val_accuracy: 0.4360\n",
            "Epoch 666/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.3922e-04 - accuracy: 0.9999 - val_loss: 12.0931 - val_accuracy: 0.4349\n",
            "Epoch 667/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 12.1110 - val_accuracy: 0.4351\n",
            "Epoch 668/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 12.0983 - val_accuracy: 0.4360\n",
            "Epoch 669/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.0765 - val_accuracy: 0.4351\n",
            "Epoch 670/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 12.0734 - val_accuracy: 0.4346\n",
            "Epoch 671/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.0907 - val_accuracy: 0.4349\n",
            "Epoch 672/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.3356e-04 - accuracy: 0.9999 - val_loss: 12.0609 - val_accuracy: 0.4351\n",
            "Epoch 673/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.0790 - val_accuracy: 0.4357\n",
            "Epoch 674/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.7224e-04 - accuracy: 0.9999 - val_loss: 12.0486 - val_accuracy: 0.4351\n",
            "Epoch 675/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.0669 - val_accuracy: 0.4351\n",
            "Epoch 676/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 12.0365 - val_accuracy: 0.4354\n",
            "Epoch 677/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 12.0547 - val_accuracy: 0.4351\n",
            "Epoch 678/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 12.0377 - val_accuracy: 0.4373\n",
            "Epoch 679/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 12.0259 - val_accuracy: 0.4378\n",
            "Epoch 680/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 12.0223 - val_accuracy: 0.4378\n",
            "Epoch 681/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 12.0186 - val_accuracy: 0.4370\n",
            "Epoch 682/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 12.0086 - val_accuracy: 0.4370\n",
            "Epoch 683/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 7.5430e-04 - accuracy: 0.9999 - val_loss: 11.9961 - val_accuracy: 0.4365\n",
            "Epoch 684/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.9881 - val_accuracy: 0.4368\n",
            "Epoch 685/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 12.0075 - val_accuracy: 0.4357\n",
            "Epoch 686/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.9727 - val_accuracy: 0.4351\n",
            "Epoch 687/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.9666 - val_accuracy: 0.4368\n",
            "Epoch 688/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.9643 - val_accuracy: 0.4362\n",
            "Epoch 689/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.9545 - val_accuracy: 0.4362\n",
            "Epoch 690/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.9666 - val_accuracy: 0.4354\n",
            "Epoch 691/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 11.9481 - val_accuracy: 0.4362\n",
            "Epoch 692/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 11.9484 - val_accuracy: 0.4357\n",
            "Epoch 693/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.9325 - val_accuracy: 0.4354\n",
            "Epoch 694/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.9503 - val_accuracy: 0.4354\n",
            "Epoch 695/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.9327 - val_accuracy: 0.4360\n",
            "Epoch 696/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.9338 - val_accuracy: 0.4346\n",
            "Epoch 697/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.9145 - val_accuracy: 0.4362\n",
            "Epoch 698/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 11.9316 - val_accuracy: 0.4362\n",
            "Epoch 699/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 8.6731e-04 - accuracy: 0.9999 - val_loss: 11.9019 - val_accuracy: 0.4346\n",
            "Epoch 700/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 11.9126 - val_accuracy: 0.4362\n",
            "Epoch 701/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.8989 - val_accuracy: 0.4362\n",
            "Epoch 702/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 8.4168e-04 - accuracy: 0.9999 - val_loss: 11.8887 - val_accuracy: 0.4357\n",
            "Epoch 703/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.9088 - val_accuracy: 0.4362\n",
            "Epoch 704/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8735 - val_accuracy: 0.4373\n",
            "Epoch 705/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8768 - val_accuracy: 0.4378\n",
            "Epoch 706/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.8681 - val_accuracy: 0.4381\n",
            "Epoch 707/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 11.8787 - val_accuracy: 0.4365\n",
            "Epoch 708/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8692 - val_accuracy: 0.4373\n",
            "Epoch 709/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.8676 - val_accuracy: 0.4373\n",
            "Epoch 710/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.8745 - val_accuracy: 0.4373\n",
            "Epoch 711/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 9.1672e-04 - accuracy: 0.9999 - val_loss: 11.8538 - val_accuracy: 0.4365\n",
            "Epoch 712/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8514 - val_accuracy: 0.4362\n",
            "Epoch 713/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 11.8518 - val_accuracy: 0.4368\n",
            "Epoch 714/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8507 - val_accuracy: 0.4357\n",
            "Epoch 715/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8535 - val_accuracy: 0.4354\n",
            "Epoch 716/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8481 - val_accuracy: 0.4349\n",
            "Epoch 717/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 11.8533 - val_accuracy: 0.4346\n",
            "Epoch 718/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.8394 - val_accuracy: 0.4351\n",
            "Epoch 719/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 11.8276 - val_accuracy: 0.4354\n",
            "Epoch 720/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 7.6086e-04 - accuracy: 0.9999 - val_loss: 11.8357 - val_accuracy: 0.4349\n",
            "Epoch 721/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.8281 - val_accuracy: 0.4354\n",
            "Epoch 722/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 9.7711e-04 - accuracy: 0.9999 - val_loss: 11.8348 - val_accuracy: 0.4349\n",
            "Epoch 723/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 11.8421 - val_accuracy: 0.4335\n",
            "Epoch 724/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 11.8352 - val_accuracy: 0.4338\n",
            "Epoch 725/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8303 - val_accuracy: 0.4346\n",
            "Epoch 726/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 11.8174 - val_accuracy: 0.4341\n",
            "Epoch 727/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8205 - val_accuracy: 0.4343\n",
            "Epoch 728/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.8206 - val_accuracy: 0.4349\n",
            "Epoch 729/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 11.8283 - val_accuracy: 0.4346\n",
            "Epoch 730/1000\n",
            "157/157 [==============================] - 10s 66ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8097 - val_accuracy: 0.4341\n",
            "Epoch 731/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.8225 - val_accuracy: 0.4341\n",
            "Epoch 732/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 11.8106 - val_accuracy: 0.4338\n",
            "Epoch 733/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 11.8113 - val_accuracy: 0.4335\n",
            "Epoch 734/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 11.8043 - val_accuracy: 0.4341\n",
            "Epoch 735/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 11.8055 - val_accuracy: 0.4341\n",
            "Epoch 736/1000\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 11.8051 - val_accuracy: 0.4343\n",
            "Epoch 737/1000\n",
            "157/157 [==============================] - 10s 65ms/step - loss: 7.5600e-04 - accuracy: 0.9999 - val_loss: 11.8206 - val_accuracy: 0.4346\n",
            "Epoch 738/1000\n",
            " 27/157 [====>.........................] - ETA: 6s - loss: 1.6292e-08 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, X_labels, batch_size=100, epochs=1000, validation_data=(Y_test,Y_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lq6csrJTtKI"
      },
      "source": [
        "# Resultat VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbCoChza8Mta",
        "outputId": "87b0c3db-a39d-4854-cbf1-b3e9dcc242c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "117/117 - 7s - loss: 5.0034 - accuracy: 0.5630 - 7s/epoch - 61ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[5.003390789031982, 0.5629709362983704]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(Y_test, Y_labels, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZPiCaJh1FoE"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/Adience/VGG19+SVM_Adience_DB.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGVCGB8t8N_2"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktHY09aK8SFN",
        "outputId": "bb465c36-299e-4573-9b03-ae4b28429281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keract\n",
            "  Downloading keract-4.5.1-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: keract\n",
            "Successfully installed keract-4.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install keract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHytTnuKqOEc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "e3c9e187-1d0d-42ac-dfe2-357b6b78304b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-10f439a04d25>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Adience/VGG19+SVM_Adience_DB.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/MyDrive/Adience/VGG19+SVM_Adience_DB.h5"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "model=keras.models.load_model('/content/drive/MyDrive/Adience/VGG19+SVM_Adience_DB.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpSGnEDy8Vog"
      },
      "outputs": [],
      "source": [
        "from keract import get_activations\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF3JfyeD8WfS"
      },
      "outputs": [],
      "source": [
        "activations1 = get_activations(model,X_train[:1000],layer_names='flatten')\n",
        "activations2 = get_activations(model,X_train[1000:2000],layer_names='flatten')\n",
        "activations3 = get_activations(model,X_train[2000:3000],layer_names='flatten')\n",
        "activations4 = get_activations(model,X_train[3000:4000],layer_names='flatten')\n",
        "activations5 = get_activations(model,X_train[4000:5000],layer_names='flatten')\n",
        "activations6 = get_activations(model,X_train[5000:6000],layer_names='flatten')\n",
        "activations7 = get_activations(model,X_train[6000:7000],layer_names='flatten')\n",
        "activations8 = get_activations(model,X_train[7000:8000],layer_names='flatten')\n",
        "activations9 = get_activations(model,X_train[8000:9000],layer_names='flatten')\n",
        "activations10 = get_activations(model,X_train[9000:10000],layer_names='flatten')\n",
        "activations11= get_activations(model,X_train[10000:11000],layer_names='flatten')\n",
        "activations12= get_activations(model,X_train[11000:12000],layer_names='flatten')\n",
        "activations13= get_activations(model,X_train[12000:13000],layer_names='flatten')\n",
        "activations14= get_activations(model,X_train[13000:14000],layer_names='flatten')\n",
        "activations15= get_activations(model,X_train[14000:15000],layer_names='flatten')\n",
        "activations16= get_activations(model,X_train[15000:],layer_names='flatten')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KZl5u_-8XS7"
      },
      "outputs": [],
      "source": [
        "X_train1= activations1\n",
        "X_train1= X_train1[\"flatten\"]\n",
        "X_train2= activations2\n",
        "X_train2= X_train2[\"flatten\"]\n",
        "X_train3= activations3\n",
        "X_train3= X_train3[\"flatten\"]\n",
        "X_train4= activations4\n",
        "X_train4= X_train4[\"flatten\"]\n",
        "X_train5= activations5\n",
        "X_train5= X_train5[\"flatten\"]\n",
        "X_train6= activations6\n",
        "X_train6= X_train6[\"flatten\"]\n",
        "X_train7= activations7\n",
        "X_train7= X_train7[\"flatten\"]\n",
        "X_train8= activations8\n",
        "X_train8= X_train8[\"flatten\"]\n",
        "X_train9= activations9\n",
        "X_train9= X_train9[\"flatten\"]\n",
        "X_train10= activations10\n",
        "X_train10= X_train10[\"flatten\"]\n",
        "X_train11= activations11\n",
        "X_train11= X_train11[\"flatten\"]\n",
        "X_train12= activations12\n",
        "X_train12= X_train12[\"flatten\"]\n",
        "X_train13= activations13\n",
        "X_train13= X_train13[\"flatten\"]\n",
        "X_train14= activations14\n",
        "X_train14= X_train14[\"flatten\"]\n",
        "X_train15= activations15\n",
        "X_train15= X_train15[\"flatten\"]\n",
        "X_train16= activations16\n",
        "X_train16= X_train16[\"flatten\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcmKwUhgKcNT"
      },
      "outputs": [],
      "source": [
        "X_train =np.concatenate((X_train1,X_train2,X_train3,X_train4,X_train5,X_train6,X_train7,X_train8,X_train9,X_train10,X_train11,X_train12,X_train13,X_train14,X_train15,X_train16), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUq-tVM6Tdve",
        "outputId": "a29eb1a0-1875-4b1e-d332-d02504dce7ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15651, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el-ipOJlUQdk"
      },
      "outputs": [],
      "source": [
        "activations17 = get_activations(model,Y_test[:1000],layer_names='flatten')\n",
        "activations18 = get_activations(model,Y_test[1000:2000],layer_names='flatten')\n",
        "activations19 = get_activations(model,Y_test[2000:3000],layer_names='flatten')\n",
        "activations20 = get_activations(model,Y_test[3000:],layer_names='flatten')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr6o_lGmMwAC"
      },
      "outputs": [],
      "source": [
        "Y_test1= activations17\n",
        "Y_test1= Y_test1['flatten']\n",
        "Y_test2= activations18\n",
        "Y_test2= Y_test2['flatten']\n",
        "Y_test3= activations19\n",
        "Y_test3= Y_test3['flatten']\n",
        "Y_test4= activations20\n",
        "Y_test4= Y_test4['flatten']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he1SdHotMyY5"
      },
      "outputs": [],
      "source": [
        "Y_test =np.concatenate((Y_test1,Y_test2,Y_test3,Y_test4), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWa8DILk8YiW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LSMOxIg8ZeO"
      },
      "outputs": [],
      "source": [
        "model1 = SVC(kernel='rbf',probability=True, C=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "B9krkZtxTGG0",
        "outputId": "11ee40c9-81b1-4c64-88be-ba48f3d60e83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100, probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model1.fit(X_train,X_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_VrOPMM8bG4"
      },
      "outputs": [],
      "source": [
        "predictions1 = model1.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDNnqISR8cHN"
      },
      "outputs": [],
      "source": [
        "predictions2 = model1.predict(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RkPvMj38dS7"
      },
      "outputs": [],
      "source": [
        "percentage = model1.score(Y_test,Y_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "180oAIMPT53G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2xNJ4o08flt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b6b9ee-85c6-44e2-c98e-2395e96b8674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: train=99.987, test=46.878\n"
          ]
        }
      ],
      "source": [
        "score_train = accuracy_score(X_labels, predictions1)\n",
        "score_test = accuracy_score(Y_labels, predictions2)\n",
        "\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7GEvwZjT_O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad439d3f-0bcb-434c-fb9a-043296453793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 1 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "Confusion Matrix\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Test Set: 3716\n",
            "Accuracy = 46.878363832077504 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          12       1.00      1.00      1.00         1\n",
            "          13       0.00      0.00      0.00         2\n",
            "          14       1.00      1.00      1.00         4\n",
            "          15       1.00      0.50      0.67         2\n",
            "          16       1.00      0.50      0.67         2\n",
            "          17       1.00      0.75      0.86         4\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          20       1.00      1.00      1.00         1\n",
            "          21       0.00      0.00      0.00         1\n",
            "          25       1.00      0.67      0.80         3\n",
            "          27       0.17      0.33      0.22         3\n",
            "          28       1.00      0.50      0.67         2\n",
            "          29       1.00      1.00      1.00         1\n",
            "          30       0.67      0.33      0.44         6\n",
            "          31       0.06      0.10      0.07        10\n",
            "          34       0.33      0.50      0.40         2\n",
            "          38       1.00      1.00      1.00         1\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          43       1.00      1.00      1.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          48       1.00      1.00      1.00         1\n",
            "          49       0.00      0.00      0.00         1\n",
            "          52       1.00      1.00      1.00         1\n",
            "          53       1.00      1.00      1.00         1\n",
            "          54       0.00      0.00      0.00         1\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       1.00      0.29      0.44         7\n",
            "          60       0.50      0.33      0.40         6\n",
            "          61       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         1\n",
            "          68       0.00      0.00      0.00         1\n",
            "          70       0.00      0.00      0.00         1\n",
            "          73       1.00      1.00      1.00         1\n",
            "          74       0.33      0.50      0.40         2\n",
            "          76       0.00      0.00      0.00         1\n",
            "          81       0.50      0.50      0.50         2\n",
            "          84       1.00      0.50      0.67         2\n",
            "          85       0.00      0.00      0.00         1\n",
            "          88       0.00      0.00      0.00         1\n",
            "          89       0.00      0.00      0.00         2\n",
            "          90       1.00      1.00      1.00         1\n",
            "          92       1.00      1.00      1.00         1\n",
            "          93       0.00      0.00      0.00         1\n",
            "          95       1.00      1.00      1.00         1\n",
            "          97       0.00      0.00      0.00         1\n",
            "          99       0.00      0.00      0.00         1\n",
            "         108       0.00      0.00      0.00         1\n",
            "         110       0.00      0.00      0.00         1\n",
            "         111       0.00      0.00      0.00         2\n",
            "         113       0.00      0.00      0.00         1\n",
            "         116       1.00      1.00      1.00         1\n",
            "         117       0.50      0.56      0.53         9\n",
            "         118       1.00      0.50      0.67         2\n",
            "         119       0.19      0.40      0.25        20\n",
            "         120       0.00      0.00      0.00         2\n",
            "         122       1.00      1.00      1.00         1\n",
            "         123       1.00      1.00      1.00         2\n",
            "         124       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         2\n",
            "         128       0.00      0.00      0.00         2\n",
            "         132       1.00      1.00      1.00         1\n",
            "         134       0.00      0.00      0.00         1\n",
            "         135       1.00      1.00      1.00         1\n",
            "         143       0.19      0.36      0.25        11\n",
            "         144       0.27      0.25      0.26        12\n",
            "         146       0.00      0.00      0.00         2\n",
            "         148       0.67      1.00      0.80         2\n",
            "         151       0.00      0.00      0.00         1\n",
            "         153       0.00      0.00      0.00         2\n",
            "         154       0.00      0.00      0.00         1\n",
            "         155       0.00      0.00      0.00         1\n",
            "         158       0.00      0.00      0.00         1\n",
            "         159       1.00      1.00      1.00         1\n",
            "         160       1.00      1.00      1.00         2\n",
            "         161       1.00      1.00      1.00         1\n",
            "         162       0.00      0.00      0.00         2\n",
            "         163       0.00      0.00      0.00         2\n",
            "         164       1.00      1.00      1.00         1\n",
            "         165       0.00      0.00      0.00         1\n",
            "         166       0.00      0.00      0.00         1\n",
            "         169       1.00      1.00      1.00         1\n",
            "         171       0.00      0.00      0.00         2\n",
            "         173       0.00      0.00      0.00         1\n",
            "         177       1.00      1.00      1.00         1\n",
            "         179       1.00      0.50      0.67         6\n",
            "         180       0.00      0.00      0.00         1\n",
            "         182       0.00      0.00      0.00         1\n",
            "         183       0.00      0.00      0.00         1\n",
            "         186       1.00      1.00      1.00         1\n",
            "         187       0.50      0.50      0.50        12\n",
            "         188       0.67      0.67      0.67         3\n",
            "         189       0.75      0.67      0.71         9\n",
            "         190       0.50      0.57      0.53         7\n",
            "         191       0.00      0.00      0.00         1\n",
            "         192       0.00      0.00      0.00         2\n",
            "         194       0.00      0.00      0.00         2\n",
            "         195       1.00      0.67      0.80         6\n",
            "         196       0.00      0.00      0.00         1\n",
            "         199       0.00      0.00      0.00         1\n",
            "         203       0.90      0.64      0.75        14\n",
            "         204       0.60      0.33      0.43         9\n",
            "         205       0.00      0.00      0.00         2\n",
            "         206       0.00      0.00      0.00         2\n",
            "         211       0.63      0.71      0.67        17\n",
            "         212       1.00      0.50      0.67         2\n",
            "         214       1.00      0.67      0.80         6\n",
            "         215       0.00      0.00      0.00         2\n",
            "         216       0.00      0.00      0.00         2\n",
            "         217       0.00      0.00      0.00         1\n",
            "         219       0.00      0.00      0.00         1\n",
            "         224       0.00      0.00      0.00         1\n",
            "         237       1.00      0.67      0.80         3\n",
            "         239       0.28      0.86      0.43        29\n",
            "         240       1.00      1.00      1.00         1\n",
            "         241       1.00      0.33      0.50         3\n",
            "         243       0.62      0.62      0.62         8\n",
            "         245       0.67      0.67      0.67         6\n",
            "         246       0.67      0.67      0.67         3\n",
            "         248       1.00      1.00      1.00         1\n",
            "         249       1.00      0.25      0.40         4\n",
            "         250       0.00      0.00      0.00         1\n",
            "         256       1.00      0.67      0.80         3\n",
            "         257       1.00      1.00      1.00         1\n",
            "         259       1.00      1.00      1.00         2\n",
            "         260       1.00      1.00      1.00         2\n",
            "         261       0.00      0.00      0.00         3\n",
            "         262       1.00      1.00      1.00         1\n",
            "         265       0.00      0.00      0.00         3\n",
            "         266       1.00      1.00      1.00         1\n",
            "         267       1.00      1.00      1.00         1\n",
            "         268       1.00      1.00      1.00         2\n",
            "         269       1.00      1.00      1.00         1\n",
            "         270       1.00      0.50      0.67         2\n",
            "         272       1.00      1.00      1.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         279       0.33      0.48      0.39        44\n",
            "         280       0.50      0.50      0.50         6\n",
            "         281       0.00      0.00      0.00         1\n",
            "         283       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00         6\n",
            "         286       0.00      0.00      0.00         1\n",
            "         288       0.29      0.86      0.44        14\n",
            "         289       1.00      1.00      1.00         2\n",
            "         291       0.00      0.00      0.00         2\n",
            "         292       0.23      0.20      0.21        15\n",
            "         293       0.00      0.00      0.00         1\n",
            "         297       0.00      0.00      0.00         1\n",
            "         298       0.50      0.50      0.50         6\n",
            "         299       0.00      0.00      0.00         3\n",
            "         300       1.00      0.50      0.67         2\n",
            "         302       1.00      1.00      1.00         1\n",
            "         303       0.00      0.00      0.00         1\n",
            "         305       1.00      0.33      0.50         3\n",
            "         307       0.50      0.67      0.57         3\n",
            "         308       1.00      1.00      1.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       1.00      1.00      1.00         1\n",
            "         311       0.00      0.00      0.00         2\n",
            "         312       0.00      0.00      0.00         1\n",
            "         314       0.00      0.00      0.00         1\n",
            "         316       0.00      0.00      0.00         1\n",
            "         318       1.00      1.00      1.00         1\n",
            "         320       0.00      0.00      0.00         1\n",
            "         321       0.00      0.00      0.00         2\n",
            "         322       0.00      0.00      0.00         1\n",
            "         323       0.00      0.00      0.00         1\n",
            "         326       0.00      0.00      0.00         2\n",
            "         327       1.00      1.00      1.00         1\n",
            "         328       1.00      1.00      1.00         1\n",
            "         329       1.00      1.00      1.00         1\n",
            "         330       1.00      1.00      1.00         1\n",
            "         332       0.00      0.00      0.00         1\n",
            "         333       0.00      0.00      0.00         1\n",
            "         339       0.00      0.00      0.00         1\n",
            "         341       0.00      0.00      0.00         1\n",
            "         347       0.87      0.81      0.84        16\n",
            "         350       0.00      0.00      0.00         1\n",
            "         351       1.00      1.00      1.00         2\n",
            "         352       0.00      0.00      0.00         2\n",
            "         353       0.33      0.33      0.33         3\n",
            "         354       1.00      0.50      0.67         2\n",
            "         356       1.00      0.33      0.50         3\n",
            "         357       0.33      0.33      0.33         3\n",
            "         358       0.00      0.00      0.00         2\n",
            "         359       0.00      0.00      0.00         0\n",
            "         360       0.00      0.00      0.00         2\n",
            "         361       1.00      0.50      0.67         2\n",
            "         362       0.00      0.00      0.00         4\n",
            "         363       0.00      0.00      0.00         2\n",
            "         364       0.00      0.00      0.00         3\n",
            "         365       1.00      1.00      1.00         1\n",
            "         366       0.50      1.00      0.67         1\n",
            "         367       0.67      0.40      0.50         5\n",
            "         369       1.00      1.00      1.00         1\n",
            "         370       0.00      0.00      0.00         1\n",
            "         371       1.00      1.00      1.00         2\n",
            "         372       1.00      0.50      0.67         2\n",
            "         373       1.00      1.00      1.00         1\n",
            "         374       1.00      0.50      0.67         2\n",
            "         375       0.00      0.00      0.00         1\n",
            "         376       1.00      1.00      1.00         2\n",
            "         377       1.00      1.00      1.00         2\n",
            "         380       0.00      0.00      0.00         1\n",
            "         382       0.00      0.00      0.00         1\n",
            "         383       0.00      0.00      0.00         1\n",
            "         386       0.00      0.00      0.00         2\n",
            "         396       0.00      0.00      0.00         2\n",
            "         397       0.00      0.00      0.00         2\n",
            "         398       0.38      0.50      0.43         6\n",
            "         399       1.00      1.00      1.00         1\n",
            "         400       0.50      0.50      0.50         2\n",
            "         403       1.00      1.00      1.00         2\n",
            "         407       0.00      0.00      0.00         2\n",
            "         408       0.00      0.00      0.00         2\n",
            "         410       0.00      0.00      0.00         1\n",
            "         412       0.00      0.00      0.00         1\n",
            "         413       0.00      0.00      0.00         1\n",
            "         416       1.00      1.00      1.00         1\n",
            "         422       0.00      0.00      0.00         2\n",
            "         423       0.00      0.00      0.00         1\n",
            "         424       0.00      0.00      0.00         2\n",
            "         427       0.00      0.00      0.00         1\n",
            "         437       0.00      0.00      0.00         1\n",
            "         444       0.00      0.00      0.00         1\n",
            "         445       0.00      0.00      0.00         1\n",
            "         448       0.00      0.00      0.00         1\n",
            "         450       0.00      0.00      0.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         456       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         1\n",
            "         459       0.00      0.00      0.00         1\n",
            "         463       0.00      0.00      0.00         1\n",
            "         464       1.00      1.00      1.00         1\n",
            "         468       1.00      1.00      1.00         1\n",
            "         474       1.00      1.00      1.00         2\n",
            "         475       0.80      1.00      0.89         4\n",
            "         476       0.50      0.50      0.50         2\n",
            "         479       1.00      1.00      1.00         2\n",
            "         485       0.00      0.00      0.00         1\n",
            "         490       0.00      0.00      0.00         1\n",
            "         492       0.00      0.00      0.00         1\n",
            "         494       0.00      0.00      0.00         1\n",
            "         496       0.00      0.00      0.00         2\n",
            "         498       0.00      0.00      0.00         1\n",
            "         502       0.00      0.00      0.00         1\n",
            "         503       0.00      0.00      0.00         2\n",
            "         504       0.00      0.00      0.00         1\n",
            "         505       0.00      0.00      0.00         1\n",
            "         507       0.00      0.00      0.00         0\n",
            "         509       0.00      0.00      0.00         1\n",
            "         514       1.00      0.50      0.67         2\n",
            "         515       0.00      0.00      0.00         1\n",
            "         516       0.00      0.00      0.00         0\n",
            "         521       0.00      0.00      0.00         2\n",
            "         524       0.00      0.00      0.00         1\n",
            "         525       0.00      0.00      0.00         1\n",
            "         537       0.00      0.00      0.00         1\n",
            "         553       1.00      0.67      0.80         3\n",
            "         554       1.00      1.00      1.00         1\n",
            "         555       1.00      1.00      1.00         1\n",
            "         556       1.00      1.00      1.00         1\n",
            "         557       1.00      1.00      1.00         1\n",
            "         558       1.00      1.00      1.00         1\n",
            "         559       0.00      0.00      0.00         1\n",
            "         560       1.00      1.00      1.00         4\n",
            "         561       1.00      1.00      1.00         1\n",
            "         562       1.00      1.00      1.00         1\n",
            "         563       1.00      1.00      1.00         1\n",
            "         564       1.00      1.00      1.00         1\n",
            "         565       1.00      1.00      1.00         1\n",
            "         566       1.00      1.00      1.00         1\n",
            "         567       1.00      1.00      1.00         2\n",
            "         570       1.00      1.00      1.00         1\n",
            "         571       0.00      0.00      0.00         2\n",
            "         572       1.00      1.00      1.00         2\n",
            "         573       1.00      1.00      1.00         2\n",
            "         574       1.00      1.00      1.00         2\n",
            "         577       1.00      1.00      1.00         2\n",
            "         579       1.00      1.00      1.00         1\n",
            "         581       0.00      0.00      0.00         1\n",
            "         582       0.00      0.00      0.00         1\n",
            "         584       0.25      0.20      0.22         5\n",
            "         585       0.00      0.00      0.00         3\n",
            "         586       0.00      0.00      0.00         3\n",
            "         587       0.00      0.00      0.00         0\n",
            "         588       0.00      0.00      0.00         1\n",
            "         590       0.00      0.00      0.00         1\n",
            "         592       0.00      0.00      0.00         1\n",
            "         593       0.00      0.00      0.00         2\n",
            "         594       0.00      0.00      0.00         1\n",
            "         595       1.00      0.50      0.67         4\n",
            "         596       0.00      0.00      0.00         1\n",
            "         597       0.00      0.00      0.00         2\n",
            "         600       0.00      0.00      0.00         1\n",
            "         601       0.00      0.00      0.00         1\n",
            "         603       0.00      0.00      0.00         2\n",
            "         604       0.00      0.00      0.00         1\n",
            "         608       0.00      0.00      0.00         1\n",
            "         609       0.00      0.00      0.00         1\n",
            "         611       0.00      0.00      0.00         1\n",
            "         613       0.00      0.00      0.00         1\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       0.00      0.00      0.00         1\n",
            "         618       0.00      0.00      0.00         1\n",
            "         619       0.00      0.00      0.00         1\n",
            "         629       1.00      0.50      0.67         2\n",
            "         631       0.00      0.00      0.00         1\n",
            "         632       0.00      0.00      0.00         1\n",
            "         634       1.00      0.50      0.67         2\n",
            "         635       0.00      0.00      0.00         2\n",
            "         636       1.00      1.00      1.00         1\n",
            "         642       1.00      0.67      0.80         6\n",
            "         643       1.00      0.50      0.67         6\n",
            "         644       1.00      1.00      1.00         3\n",
            "         645       1.00      0.67      0.80         3\n",
            "         648       1.00      0.20      0.33         5\n",
            "         649       0.00      0.00      0.00         1\n",
            "         650       1.00      1.00      1.00         1\n",
            "         651       1.00      1.00      1.00         2\n",
            "         653       0.33      0.17      0.22         6\n",
            "         654       1.00      1.00      1.00         1\n",
            "         661       1.00      1.00      1.00         1\n",
            "         662       0.00      0.00      0.00         1\n",
            "         666       1.00      0.50      0.67         2\n",
            "         667       1.00      1.00      1.00         1\n",
            "         669       0.00      0.00      0.00         1\n",
            "         672       0.00      0.00      0.00         1\n",
            "         678       0.00      0.00      0.00         2\n",
            "         680       0.50      0.20      0.29         5\n",
            "         681       1.00      0.50      0.67         2\n",
            "         682       1.00      1.00      1.00         1\n",
            "         683       0.33      0.20      0.25         5\n",
            "         684       0.00      0.00      0.00         1\n",
            "         685       0.42      0.56      0.48         9\n",
            "         688       1.00      1.00      1.00         2\n",
            "         690       1.00      0.67      0.80         3\n",
            "         691       1.00      0.75      0.86         4\n",
            "         692       1.00      1.00      1.00         1\n",
            "         693       1.00      1.00      1.00         1\n",
            "         695       0.00      0.00      0.00         1\n",
            "         698       1.00      1.00      1.00         1\n",
            "         699       0.00      0.00      0.00         1\n",
            "         701       0.00      0.00      0.00         1\n",
            "         703       0.00      0.00      0.00         0\n",
            "         705       0.00      0.00      0.00         1\n",
            "         706       1.00      0.50      0.67         4\n",
            "         707       0.00      0.00      0.00         2\n",
            "         708       1.00      0.33      0.50         3\n",
            "         710       0.00      0.00      0.00         1\n",
            "         712       0.00      0.00      0.00         1\n",
            "         713       0.00      0.00      0.00         1\n",
            "         714       1.00      1.00      1.00         1\n",
            "         715       0.00      0.00      0.00         1\n",
            "         716       0.00      0.00      0.00         1\n",
            "         718       0.00      0.00      0.00         1\n",
            "         719       0.00      0.00      0.00         1\n",
            "         723       0.00      0.00      0.00         1\n",
            "         724       0.00      0.00      0.00         1\n",
            "         725       1.00      1.00      1.00         1\n",
            "         727       1.00      1.00      1.00         1\n",
            "         729       0.00      0.00      0.00         1\n",
            "         730       1.00      1.00      1.00         1\n",
            "         731       1.00      1.00      1.00         1\n",
            "         733       0.00      0.00      0.00         1\n",
            "         739       0.00      0.00      0.00         1\n",
            "         744       0.00      0.00      0.00         1\n",
            "         745       0.00      0.00      0.00         1\n",
            "         747       0.00      0.00      0.00         1\n",
            "         749       0.50      1.00      0.67         1\n",
            "         751       0.00      0.00      0.00         2\n",
            "         752       0.00      0.00      0.00         1\n",
            "         753       0.00      0.00      0.00         1\n",
            "         756       0.00      0.00      0.00         1\n",
            "         757       0.50      0.17      0.25         6\n",
            "         758       0.00      0.00      0.00         4\n",
            "         761       0.00      0.00      0.00         1\n",
            "         763       0.00      0.00      0.00         1\n",
            "         771       0.00      0.00      0.00         1\n",
            "         773       0.00      0.00      0.00         1\n",
            "         775       0.00      0.00      0.00         1\n",
            "         776       0.00      0.00      0.00         1\n",
            "         777       0.00      0.00      0.00         1\n",
            "         778       0.00      0.00      0.00         1\n",
            "         780       0.00      0.00      0.00         2\n",
            "         781       0.00      0.00      0.00         1\n",
            "         784       0.00      0.00      0.00         1\n",
            "         786       1.00      0.50      0.67         2\n",
            "         788       0.00      0.00      0.00         1\n",
            "         791       0.57      0.40      0.47        10\n",
            "         792       0.00      0.00      0.00         2\n",
            "         793       1.00      0.33      0.50         6\n",
            "         795       0.00      0.00      0.00         1\n",
            "         800       0.00      0.00      0.00         1\n",
            "         802       0.00      0.00      0.00         1\n",
            "         803       0.00      0.00      0.00         1\n",
            "         804       0.00      0.00      0.00         1\n",
            "         809       0.20      0.08      0.12        12\n",
            "         810       0.00      0.00      0.00         1\n",
            "         814       0.00      0.00      0.00         1\n",
            "         816       0.00      0.00      0.00         3\n",
            "         817       0.00      0.00      0.00         2\n",
            "         820       0.57      0.67      0.62         6\n",
            "         821       0.00      0.00      0.00         1\n",
            "         825       0.00      0.00      0.00         1\n",
            "         826       1.00      1.00      1.00         1\n",
            "         827       0.50      0.64      0.56        11\n",
            "         828       0.00      0.00      0.00         1\n",
            "         829       1.00      0.25      0.40         4\n",
            "         830       0.00      0.00      0.00         2\n",
            "         831       0.00      0.00      0.00         2\n",
            "         835       1.00      1.00      1.00         1\n",
            "         836       1.00      1.00      1.00         1\n",
            "         837       1.00      1.00      1.00         1\n",
            "         838       1.00      1.00      1.00         1\n",
            "         839       1.00      1.00      1.00         2\n",
            "         840       1.00      0.67      0.80         3\n",
            "         841       0.50      0.50      0.50         6\n",
            "         842       0.50      0.67      0.57         3\n",
            "         843       1.00      1.00      1.00         3\n",
            "         844       1.00      0.50      0.67         2\n",
            "         846       0.00      0.00      0.00         1\n",
            "         847       1.00      1.00      1.00         1\n",
            "         848       0.00      0.00      0.00         1\n",
            "         849       0.00      0.00      0.00         2\n",
            "         851       1.00      1.00      1.00         2\n",
            "         852       0.50      1.00      0.67         1\n",
            "         854       0.00      0.00      0.00         1\n",
            "         855       0.00      0.00      0.00         1\n",
            "         856       0.00      0.00      0.00         2\n",
            "         857       0.75      0.50      0.60         6\n",
            "         858       0.57      0.67      0.62         6\n",
            "         859       0.67      0.67      0.67         6\n",
            "         860       0.50      0.33      0.40         3\n",
            "         869       1.00      1.00      1.00         3\n",
            "         870       0.00      0.00      0.00         3\n",
            "         871       0.00      0.00      0.00         3\n",
            "         873       1.00      1.00      1.00         2\n",
            "         874       0.00      0.00      0.00         2\n",
            "         877       0.50      0.17      0.25         6\n",
            "         878       0.00      0.00      0.00         1\n",
            "         879       0.00      0.00      0.00         3\n",
            "         881       0.00      0.00      0.00         1\n",
            "         883       0.00      0.00      0.00         2\n",
            "         884       1.00      1.00      1.00         2\n",
            "         885       0.00      0.00      0.00         1\n",
            "         886       0.00      0.00      0.00         2\n",
            "         889       0.00      0.00      0.00         1\n",
            "         891       1.00      1.00      1.00         1\n",
            "         899       0.00      0.00      0.00         1\n",
            "         904       1.00      1.00      1.00         1\n",
            "         923       1.00      1.00      1.00         2\n",
            "         924       1.00      0.50      0.67         2\n",
            "         926       1.00      1.00      1.00         1\n",
            "         938       0.00      0.00      0.00         1\n",
            "         948       0.00      0.00      0.00         2\n",
            "         949       0.00      0.00      0.00         2\n",
            "         950       0.00      0.00      0.00         1\n",
            "         957       1.00      1.00      1.00         1\n",
            "         961       0.00      0.00      0.00         1\n",
            "         970       1.00      0.50      0.67         2\n",
            "         977       1.00      1.00      1.00         1\n",
            "         978       0.00      0.00      0.00         1\n",
            "         979       0.00      0.00      0.00         1\n",
            "         981       0.00      0.00      0.00         1\n",
            "         982       1.00      1.00      1.00         1\n",
            "         984       0.00      0.00      0.00         1\n",
            "         988       0.00      0.00      0.00         1\n",
            "         989       1.00      1.00      1.00         1\n",
            "         993       0.00      0.00      0.00         1\n",
            "         996       0.00      0.00      0.00         1\n",
            "         998       0.00      0.00      0.00         1\n",
            "         999       1.00      1.00      1.00         1\n",
            "        1001       0.00      0.00      0.00         1\n",
            "        1004       0.00      0.00      0.00         1\n",
            "        1005       0.00      0.00      0.00         1\n",
            "        1009       1.00      1.00      1.00         1\n",
            "        1010       0.00      0.00      0.00         1\n",
            "        1015       0.00      0.00      0.00         1\n",
            "        1016       0.00      0.00      0.00         1\n",
            "        1019       0.00      0.00      0.00         1\n",
            "        1020       1.00      1.00      1.00         1\n",
            "        1032       0.00      0.00      0.00         1\n",
            "        1033       0.00      0.00      0.00         1\n",
            "        1034       0.00      0.00      0.00         2\n",
            "        1035       0.00      0.00      0.00         2\n",
            "        1036       1.00      0.67      0.80         6\n",
            "        1038       0.00      0.00      0.00         1\n",
            "        1039       0.00      0.00      0.00         1\n",
            "        1040       0.00      0.00      0.00         1\n",
            "        1043       0.00      0.00      0.00         1\n",
            "        1045       0.00      0.00      0.00         1\n",
            "        1048       0.00      0.00      0.00         1\n",
            "        1050       0.29      0.62      0.39        13\n",
            "        1051       0.00      0.00      0.00         2\n",
            "        1052       0.83      0.83      0.83         6\n",
            "        1053       0.00      0.00      0.00         1\n",
            "        1056       1.00      1.00      1.00         1\n",
            "        1057       0.00      0.00      0.00         1\n",
            "        1059       1.00      1.00      1.00         1\n",
            "        1062       0.00      0.00      0.00         1\n",
            "        1064       0.00      0.00      0.00         1\n",
            "        1071       0.00      0.00      0.00         1\n",
            "        1073       1.00      1.00      1.00         1\n",
            "        1075       0.00      0.00      0.00         1\n",
            "        1081       0.27      0.31      0.29        13\n",
            "        1082       0.14      0.18      0.16        11\n",
            "        1083       0.00      0.00      0.00         4\n",
            "        1084       1.00      0.50      0.67         2\n",
            "        1085       1.00      0.33      0.50         3\n",
            "        1086       1.00      0.50      0.67         2\n",
            "        1088       1.00      0.75      0.86         4\n",
            "        1089       1.00      1.00      1.00         2\n",
            "        1090       0.62      1.00      0.77         5\n",
            "        1092       1.00      0.67      0.80         3\n",
            "        1094       0.00      0.00      0.00         3\n",
            "        1095       1.00      1.00      1.00         2\n",
            "        1096       1.00      1.00      1.00         1\n",
            "        1097       1.00      1.00      1.00         1\n",
            "        1098       1.00      1.00      1.00         1\n",
            "        1099       1.00      1.00      1.00         1\n",
            "        1100       1.00      1.00      1.00         1\n",
            "        1101       1.00      1.00      1.00         1\n",
            "        1102       1.00      1.00      1.00         1\n",
            "        1103       1.00      1.00      1.00         1\n",
            "        1104       1.00      1.00      1.00         1\n",
            "        1105       0.00      0.00      0.00         1\n",
            "        1106       1.00      1.00      1.00         1\n",
            "        1107       0.00      0.00      0.00         1\n",
            "        1108       1.00      1.00      1.00         1\n",
            "        1110       1.00      1.00      1.00         1\n",
            "        1112       1.00      1.00      1.00         1\n",
            "        1114       1.00      1.00      1.00         1\n",
            "        1117       0.39      0.79      0.53        19\n",
            "        1118       1.00      1.00      1.00         1\n",
            "        1121       1.00      1.00      1.00         2\n",
            "        1122       0.00      0.00      0.00         1\n",
            "        1128       1.00      1.00      1.00         2\n",
            "        1131       0.00      0.00      0.00         2\n",
            "        1139       0.00      0.00      0.00         1\n",
            "        1140       0.00      0.00      0.00         1\n",
            "        1142       0.00      0.00      0.00         1\n",
            "        1143       0.00      0.00      0.00         1\n",
            "        1144       0.00      0.00      0.00         1\n",
            "        1145       0.00      0.00      0.00         1\n",
            "        1146       0.00      0.00      0.00         1\n",
            "        1148       0.00      0.00      0.00         1\n",
            "        1150       0.00      0.00      0.00         1\n",
            "        1151       0.00      0.00      0.00         1\n",
            "        1155       0.00      0.00      0.00         1\n",
            "        1156       0.00      0.00      0.00         1\n",
            "        1161       0.00      0.00      0.00         1\n",
            "        1162       0.00      0.00      0.00         1\n",
            "        1166       0.00      0.00      0.00         1\n",
            "        1167       1.00      1.00      1.00         1\n",
            "        1168       1.00      1.00      1.00         1\n",
            "        1169       0.00      0.00      0.00         2\n",
            "        1170       0.00      0.00      0.00         1\n",
            "        1171       0.00      0.00      0.00         1\n",
            "        1174       0.00      0.00      0.00         1\n",
            "        1177       0.00      0.00      0.00         1\n",
            "        1180       0.00      0.00      0.00         1\n",
            "        1183       0.00      0.00      0.00         1\n",
            "        1195       1.00      0.67      0.80         3\n",
            "        1196       0.00      0.00      0.00         3\n",
            "        1197       1.00      1.00      1.00         1\n",
            "        1198       0.00      0.00      0.00         2\n",
            "        1200       0.25      0.12      0.17         8\n",
            "        1201       0.00      0.00      0.00         1\n",
            "        1205       0.00      0.00      0.00         1\n",
            "        1212       0.00      0.00      0.00         1\n",
            "        1226       0.00      0.00      0.00         1\n",
            "        1232       0.26      0.51      0.34        37\n",
            "        1236       1.00      0.33      0.50         3\n",
            "        1237       0.73      1.00      0.84         8\n",
            "        1238       0.00      0.00      0.00         3\n",
            "        1240       0.00      0.00      0.00         1\n",
            "        1242       0.00      0.00      0.00         1\n",
            "        1243       0.00      0.00      0.00         1\n",
            "        1248       0.41      0.57      0.48        44\n",
            "        1249       1.00      0.83      0.91         6\n",
            "        1250       0.75      0.50      0.60         6\n",
            "        1251       1.00      1.00      1.00         1\n",
            "        1252       1.00      1.00      1.00         1\n",
            "        1253       1.00      1.00      1.00         1\n",
            "        1254       1.00      1.00      1.00         1\n",
            "        1255       0.00      0.00      0.00         1\n",
            "        1256       1.00      1.00      1.00         1\n",
            "        1257       1.00      1.00      1.00         1\n",
            "        1258       0.00      0.00      0.00         1\n",
            "        1259       1.00      1.00      1.00         1\n",
            "        1262       0.00      0.00      0.00         1\n",
            "        1263       1.00      1.00      1.00         1\n",
            "        1265       0.00      0.00      0.00         1\n",
            "        1267       1.00      1.00      1.00         1\n",
            "        1268       1.00      1.00      1.00         1\n",
            "        1269       1.00      1.00      1.00         1\n",
            "        1272       1.00      0.50      0.67         2\n",
            "        1273       1.00      0.50      0.67         2\n",
            "        1275       0.00      0.00      0.00         1\n",
            "        1280       0.00      0.00      0.00         1\n",
            "        1285       1.00      1.00      1.00         2\n",
            "        1286       1.00      0.67      0.80         3\n",
            "        1287       1.00      0.67      0.80         3\n",
            "        1288       1.00      1.00      1.00         1\n",
            "        1289       0.00      0.00      0.00         1\n",
            "        1290       1.00      1.00      1.00         1\n",
            "        1292       1.00      1.00      1.00         2\n",
            "        1297       0.00      0.00      0.00         1\n",
            "        1299       0.00      0.00      0.00         1\n",
            "        1300       0.00      0.00      0.00         2\n",
            "        1301       0.00      0.00      0.00         1\n",
            "        1302       1.00      1.00      1.00         1\n",
            "        1303       0.00      0.00      0.00         1\n",
            "        1304       0.00      0.00      0.00         1\n",
            "        1306       0.00      0.00      0.00         1\n",
            "        1312       1.00      1.00      1.00         1\n",
            "        1313       1.00      1.00      1.00         1\n",
            "        1318       0.00      0.00      0.00         1\n",
            "        1320       0.00      0.00      0.00         1\n",
            "        1321       0.00      0.00      0.00         4\n",
            "        1322       0.00      0.00      0.00         2\n",
            "        1323       0.00      0.00      0.00         1\n",
            "        1325       1.00      1.00      1.00         1\n",
            "        1326       1.00      1.00      1.00         1\n",
            "        1327       0.00      0.00      0.00         1\n",
            "        1328       1.00      1.00      1.00         2\n",
            "        1329       1.00      0.75      0.86         4\n",
            "        1330       1.00      1.00      1.00         1\n",
            "        1332       1.00      0.67      0.80         6\n",
            "        1333       1.00      0.67      0.80         3\n",
            "        1334       1.00      0.50      0.67         4\n",
            "        1335       1.00      0.50      0.67         4\n",
            "        1336       1.00      0.50      0.67         4\n",
            "        1337       0.00      0.00      0.00         2\n",
            "        1338       1.00      0.33      0.50         3\n",
            "        1339       0.00      0.00      0.00         1\n",
            "        1340       0.00      0.00      0.00         2\n",
            "        1341       0.25      0.50      0.33         2\n",
            "        1343       0.00      0.00      0.00         1\n",
            "        1346       0.25      0.54      0.34        24\n",
            "        1347       0.32      0.52      0.39        25\n",
            "        1348       0.33      0.67      0.44         9\n",
            "        1349       0.00      0.00      0.00         5\n",
            "        1350       1.00      1.00      1.00         1\n",
            "        1352       0.00      0.00      0.00         1\n",
            "        1353       0.00      0.00      0.00         2\n",
            "        1355       0.00      0.00      0.00         1\n",
            "        1357       0.00      0.00      0.00         1\n",
            "        1358       0.00      0.00      0.00         2\n",
            "        1362       0.00      0.00      0.00         1\n",
            "        1370       0.40      0.29      0.33         7\n",
            "        1371       0.00      0.00      0.00         3\n",
            "        1372       0.00      0.00      0.00         1\n",
            "        1374       0.00      0.00      0.00         1\n",
            "        1379       0.00      0.00      0.00         2\n",
            "        1380       0.00      0.00      0.00         1\n",
            "        1381       0.50      0.33      0.40         3\n",
            "        1383       0.00      0.00      0.00         1\n",
            "        1384       0.33      0.58      0.42        48\n",
            "        1386       0.00      0.00      0.00         2\n",
            "        1388       0.00      0.00      0.00         3\n",
            "        1389       1.00      0.50      0.67         2\n",
            "        1390       0.00      0.00      0.00         1\n",
            "        1391       1.00      1.00      1.00         2\n",
            "        1394       0.00      0.00      0.00         8\n",
            "        1395       0.60      0.50      0.55         6\n",
            "        1396       1.00      0.50      0.67         2\n",
            "        1399       0.00      0.00      0.00         1\n",
            "        1401       0.00      0.00      0.00         2\n",
            "        1404       0.00      0.00      0.00         1\n",
            "        1406       0.00      0.00      0.00         1\n",
            "        1409       0.00      0.00      0.00         1\n",
            "        1414       0.00      0.00      0.00         4\n",
            "        1415       0.00      0.00      0.00         2\n",
            "        1416       0.00      0.00      0.00         1\n",
            "        1417       0.00      0.00      0.00         1\n",
            "        1419       0.00      0.00      0.00         1\n",
            "        1420       0.00      0.00      0.00         1\n",
            "        1422       0.00      0.00      0.00         2\n",
            "        1423       0.75      0.43      0.55         7\n",
            "        1424       0.50      1.00      0.67         1\n",
            "        1428       1.00      1.00      1.00         1\n",
            "        1429       1.00      1.00      1.00         1\n",
            "        1431       0.27      0.86      0.42        83\n",
            "        1432       0.00      0.00      0.00         3\n",
            "        1433       0.12      0.12      0.12         8\n",
            "        1434       0.19      0.55      0.28        22\n",
            "        1435       0.00      0.00      0.00         1\n",
            "        1436       0.17      0.17      0.17         6\n",
            "        1437       0.20      0.50      0.29         2\n",
            "        1438       0.00      0.00      0.00         3\n",
            "        1439       0.73      0.53      0.62        15\n",
            "        1440       0.50      0.50      0.50         2\n",
            "        1442       0.00      0.00      0.00         1\n",
            "        1443       0.25      0.25      0.25         4\n",
            "        1444       0.00      0.00      0.00         4\n",
            "        1445       1.00      1.00      1.00         1\n",
            "        1447       1.00      1.00      1.00         1\n",
            "        1448       0.67      0.25      0.36         8\n",
            "        1449       0.00      0.00      0.00         3\n",
            "        1450       0.00      0.00      0.00         1\n",
            "        1452       0.00      0.00      0.00         1\n",
            "        1453       0.75      0.75      0.75         4\n",
            "        1454       1.00      1.00      1.00         2\n",
            "        1455       0.00      0.00      0.00         1\n",
            "        1456       0.00      0.00      0.00         1\n",
            "        1457       1.00      1.00      1.00         1\n",
            "        1461       0.00      0.00      0.00         1\n",
            "        1462       0.17      0.23      0.19        13\n",
            "        1463       0.00      0.00      0.00         2\n",
            "        1464       0.00      0.00      0.00         3\n",
            "        1465       1.00      1.00      1.00         1\n",
            "        1466       0.08      0.14      0.10        14\n",
            "        1467       0.00      0.00      0.00         2\n",
            "        1469       0.00      0.00      0.00         2\n",
            "        1471       0.00      0.00      0.00         1\n",
            "        1472       0.00      0.00      0.00         2\n",
            "        1474       0.75      0.50      0.60         6\n",
            "        1476       1.00      0.62      0.77         8\n",
            "        1478       0.00      0.00      0.00         1\n",
            "        1480       1.00      1.00      1.00         1\n",
            "        1483       0.00      0.00      0.00         1\n",
            "        1485       1.00      1.00      1.00         1\n",
            "        1486       0.00      0.00      0.00         2\n",
            "        1487       0.00      0.00      0.00         1\n",
            "        1489       0.00      0.00      0.00         1\n",
            "        1491       0.00      0.00      0.00         1\n",
            "        1492       0.44      0.44      0.44        16\n",
            "        1493       0.59      0.71      0.65        14\n",
            "        1494       1.00      1.00      1.00         2\n",
            "        1495       1.00      1.00      1.00         1\n",
            "        1496       1.00      1.00      1.00         5\n",
            "        1497       0.50      0.83      0.62        12\n",
            "        1498       1.00      0.50      0.67         2\n",
            "        1499       1.00      1.00      1.00         2\n",
            "        1500       0.00      0.00      0.00         1\n",
            "        1504       1.00      1.00      1.00         1\n",
            "        1506       1.00      1.00      1.00         1\n",
            "        1507       0.00      0.00      0.00         1\n",
            "        1508       1.00      1.00      1.00         1\n",
            "        1512       0.00      0.00      0.00         1\n",
            "        1513       1.00      1.00      1.00         1\n",
            "        1516       1.00      1.00      1.00         1\n",
            "        1526       1.00      1.00      1.00         1\n",
            "        1540       0.00      0.00      0.00         2\n",
            "        1541       0.00      0.00      0.00         3\n",
            "        1543       0.00      0.00      0.00         1\n",
            "        1544       0.00      0.00      0.00         1\n",
            "        1545       1.00      0.25      0.40         4\n",
            "        1546       0.00      0.00      0.00         1\n",
            "        1549       0.00      0.00      0.00         1\n",
            "        1550       0.00      0.00      0.00         1\n",
            "        1557       0.21      0.28      0.24        32\n",
            "        1558       0.00      0.00      0.00         1\n",
            "        1559       0.00      0.00      0.00         1\n",
            "        1569       0.00      0.00      0.00         1\n",
            "        1571       0.00      0.00      0.00         1\n",
            "        1573       0.00      0.00      0.00         1\n",
            "        1578       0.00      0.00      0.00         2\n",
            "        1579       0.00      0.00      0.00         1\n",
            "        1580       0.16      0.48      0.24        21\n",
            "        1581       0.50      0.33      0.40         6\n",
            "        1582       0.60      0.23      0.33        13\n",
            "        1583       0.50      0.14      0.22         7\n",
            "        1584       0.20      0.25      0.22         4\n",
            "        1585       1.00      0.50      0.67         2\n",
            "        1588       0.75      0.64      0.69        14\n",
            "        1589       0.00      0.00      0.00         1\n",
            "        1590       0.00      0.00      0.00         1\n",
            "        1592       1.00      0.33      0.50         3\n",
            "        1599       1.00      1.00      1.00         1\n",
            "        1605       0.00      0.00      0.00         5\n",
            "        1606       0.00      0.00      0.00         1\n",
            "        1607       0.00      0.00      0.00         3\n",
            "        1608       0.00      0.00      0.00         1\n",
            "        1609       0.00      0.00      0.00         2\n",
            "        1610       0.25      0.33      0.29         6\n",
            "        1612       1.00      0.50      0.67         2\n",
            "        1614       0.30      0.38      0.33         8\n",
            "        1615       0.43      0.25      0.32        12\n",
            "        1617       0.38      0.27      0.32        22\n",
            "        1618       1.00      0.50      0.67         2\n",
            "        1621       0.00      0.00      0.00         1\n",
            "        1622       0.00      0.00      0.00         3\n",
            "        1625       0.00      0.00      0.00         1\n",
            "        1627       0.00      0.00      0.00         1\n",
            "        1628       0.00      0.00      0.00         2\n",
            "        1629       0.00      0.00      0.00         1\n",
            "        1630       1.00      1.00      1.00         1\n",
            "        1631       0.00      0.00      0.00         1\n",
            "        1632       0.00      0.00      0.00         1\n",
            "        1635       0.00      0.00      0.00         1\n",
            "        1638       0.00      0.00      0.00         1\n",
            "        1640       0.00      0.00      0.00         1\n",
            "        1641       0.00      0.00      0.00         1\n",
            "        1651       0.00      0.00      0.00         1\n",
            "        1653       0.00      0.00      0.00         1\n",
            "        1657       0.46      0.76      0.57        63\n",
            "        1658       0.50      0.75      0.60         4\n",
            "        1659       0.38      0.69      0.49        13\n",
            "        1660       0.71      0.56      0.63         9\n",
            "        1661       1.00      0.80      0.89         5\n",
            "        1662       1.00      1.00      1.00         1\n",
            "        1663       1.00      1.00      1.00         4\n",
            "        1664       0.86      1.00      0.92         6\n",
            "        1665       1.00      1.00      1.00         3\n",
            "        1666       1.00      1.00      1.00         2\n",
            "        1667       1.00      1.00      1.00         3\n",
            "        1669       1.00      1.00      1.00         2\n",
            "        1670       0.00      0.00      0.00         1\n",
            "        1671       0.00      0.00      0.00         1\n",
            "        1672       0.00      0.00      0.00         1\n",
            "        1674       0.00      0.00      0.00         1\n",
            "        1675       0.00      0.00      0.00         1\n",
            "        1676       0.22      0.42      0.29        26\n",
            "        1677       0.33      0.29      0.31         7\n",
            "        1678       1.00      1.00      1.00         1\n",
            "        1680       0.33      0.75      0.46        79\n",
            "        1681       0.50      0.64      0.56        14\n",
            "        1682       0.36      0.45      0.40        11\n",
            "        1683       1.00      1.00      1.00         1\n",
            "        1684       1.00      1.00      1.00         1\n",
            "        1687       0.00      0.00      0.00         1\n",
            "        1688       0.00      0.00      0.00         1\n",
            "        1690       0.00      0.00      0.00         1\n",
            "        1691       0.33      0.33      0.33         6\n",
            "        1692       0.42      0.50      0.45        10\n",
            "        1693       0.50      0.50      0.50         2\n",
            "        1694       0.00      0.00      0.00         1\n",
            "        1698       0.00      0.00      0.00         1\n",
            "        1700       0.00      0.00      0.00         1\n",
            "        1708       0.41      0.32      0.36        28\n",
            "        1709       0.00      0.00      0.00         2\n",
            "        1710       0.35      0.44      0.39        18\n",
            "        1712       0.65      0.89      0.76        19\n",
            "        1713       0.50      0.20      0.29         5\n",
            "        1714       0.50      0.50      0.50         2\n",
            "        1715       1.00      1.00      1.00         1\n",
            "        1716       1.00      1.00      1.00         1\n",
            "        1717       1.00      1.00      1.00         2\n",
            "        1718       0.00      0.00      0.00         2\n",
            "        1719       1.00      1.00      1.00         1\n",
            "        1720       1.00      1.00      1.00         1\n",
            "        1722       1.00      1.00      1.00         2\n",
            "        1723       1.00      1.00      1.00         2\n",
            "        1724       1.00      1.00      1.00         1\n",
            "        1725       1.00      1.00      1.00         2\n",
            "        1727       0.00      0.00      0.00         3\n",
            "        1729       0.00      0.00      0.00         1\n",
            "        1730       1.00      1.00      1.00         2\n",
            "        1731       1.00      1.00      1.00         1\n",
            "        1733       1.00      1.00      1.00         1\n",
            "        1734       1.00      1.00      1.00         1\n",
            "        1737       0.00      0.00      0.00         1\n",
            "        1738       1.00      1.00      1.00         1\n",
            "        1739       0.00      0.00      0.00         1\n",
            "        1740       0.00      0.00      0.00         1\n",
            "        1741       0.00      0.00      0.00         1\n",
            "        1743       0.40      0.40      0.40         5\n",
            "        1744       1.00      0.50      0.67         4\n",
            "        1748       0.00      0.00      0.00         1\n",
            "        1750       0.00      0.00      0.00         1\n",
            "        1757       1.00      0.75      0.86         4\n",
            "        1758       1.00      0.50      0.67         2\n",
            "        1759       1.00      1.00      1.00         2\n",
            "        1760       0.86      0.86      0.86         7\n",
            "        1761       1.00      0.75      0.86         4\n",
            "        1762       1.00      1.00      1.00         3\n",
            "        1763       1.00      0.75      0.86         4\n",
            "        1764       0.00      0.00      0.00         2\n",
            "        1765       1.00      1.00      1.00         6\n",
            "        1766       0.33      0.20      0.25         5\n",
            "        1767       1.00      1.00      1.00         2\n",
            "        1770       1.00      1.00      1.00         2\n",
            "        1772       1.00      1.00      1.00         2\n",
            "        1773       0.00      0.00      0.00         3\n",
            "        1774       0.00      0.00      0.00         4\n",
            "        1775       0.00      0.00      0.00         2\n",
            "        1778       0.00      0.00      0.00         1\n",
            "        1779       0.00      0.00      0.00         1\n",
            "        1784       0.00      0.00      0.00         1\n",
            "        1787       0.32      0.60      0.42        40\n",
            "        1789       0.86      0.75      0.80         8\n",
            "        1790       1.00      1.00      1.00         1\n",
            "        1791       0.40      0.74      0.52        23\n",
            "        1793       1.00      1.00      1.00         1\n",
            "        1794       1.00      1.00      1.00         1\n",
            "        1795       1.00      0.50      0.67         2\n",
            "        1798       0.00      0.00      0.00         1\n",
            "        1799       0.00      0.00      0.00         1\n",
            "        1800       0.00      0.00      0.00         1\n",
            "        1801       1.00      1.00      1.00         2\n",
            "        1803       1.00      1.00      1.00         1\n",
            "        1804       0.00      0.00      0.00         1\n",
            "        1811       0.00      0.00      0.00         1\n",
            "        1815       1.00      1.00      1.00         1\n",
            "        1823       0.00      0.00      0.00         1\n",
            "        1825       0.00      0.00      0.00         1\n",
            "        1827       1.00      1.00      1.00         1\n",
            "        1832       0.00      0.00      0.00         1\n",
            "        1835       0.00      0.00      0.00         1\n",
            "        1840       0.00      0.00      0.00         1\n",
            "        1841       0.00      0.00      0.00         1\n",
            "        1843       0.00      0.00      0.00         1\n",
            "        1847       1.00      1.00      1.00         1\n",
            "        1851       0.00      0.00      0.00         1\n",
            "        1857       0.57      0.40      0.47        10\n",
            "        1858       1.00      1.00      1.00         2\n",
            "        1859       1.00      1.00      1.00         3\n",
            "        1860       0.67      0.44      0.53         9\n",
            "        1861       1.00      1.00      1.00         1\n",
            "        1862       0.33      0.33      0.33         3\n",
            "        1863       0.67      0.57      0.62         7\n",
            "        1864       0.00      0.00      0.00         2\n",
            "        1865       0.00      0.00      0.00         0\n",
            "        1866       0.00      0.00      0.00         1\n",
            "        1867       1.00      1.00      1.00         1\n",
            "        1868       0.00      0.00      0.00         1\n",
            "        1869       1.00      0.50      0.67         2\n",
            "        1870       0.00      0.00      0.00         1\n",
            "        1873       0.50      0.25      0.33         4\n",
            "        1875       1.00      1.00      1.00         1\n",
            "        1876       1.00      1.00      1.00         1\n",
            "        1878       1.00      0.25      0.40         4\n",
            "        1879       0.00      0.00      0.00         2\n",
            "        1880       0.00      0.00      0.00         4\n",
            "        1882       0.00      0.00      0.00         1\n",
            "        1883       1.00      0.25      0.40         4\n",
            "        1884       1.00      1.00      1.00         2\n",
            "        1886       1.00      0.33      0.50         3\n",
            "        1888       0.00      0.00      0.00         1\n",
            "        1889       0.00      0.00      0.00         1\n",
            "        1891       0.00      0.00      0.00         1\n",
            "        1894       1.00      1.00      1.00         1\n",
            "        1895       0.17      0.25      0.20         8\n",
            "        1896       0.50      0.56      0.53         9\n",
            "        1900       0.00      0.00      0.00         3\n",
            "        1901       0.00      0.00      0.00         1\n",
            "        1902       0.58      0.88      0.70         8\n",
            "        1903       0.44      0.40      0.42        10\n",
            "        1904       1.00      0.25      0.40         4\n",
            "        1905       1.00      0.62      0.77         8\n",
            "        1906       0.25      0.50      0.33         4\n",
            "        1907       0.75      0.60      0.67         5\n",
            "        1909       1.00      0.43      0.60         7\n",
            "        1910       1.00      0.43      0.60         7\n",
            "        1911       0.80      0.67      0.73         6\n",
            "        1926       1.00      1.00      1.00         1\n",
            "        1927       1.00      1.00      1.00         1\n",
            "        1928       0.00      0.00      0.00         2\n",
            "        1929       0.00      0.00      0.00         2\n",
            "        1930       0.50      0.50      0.50         2\n",
            "        1932       1.00      1.00      1.00         1\n",
            "        1933       0.00      0.00      0.00         2\n",
            "        1934       1.00      1.00      1.00         1\n",
            "        1935       0.00      0.00      0.00         1\n",
            "        1936       1.00      1.00      1.00         1\n",
            "        1937       1.00      1.00      1.00         1\n",
            "        1938       0.00      0.00      0.00         2\n",
            "        1941       0.00      0.00      0.00         1\n",
            "        1942       0.00      0.00      0.00         1\n",
            "        1949       1.00      0.67      0.80         3\n",
            "        1950       0.00      0.00      0.00         1\n",
            "        1951       0.00      0.00      0.00         1\n",
            "        1952       1.00      0.50      0.67         2\n",
            "        1955       1.00      1.00      1.00         1\n",
            "        1957       1.00      0.50      0.67         8\n",
            "        1958       0.00      0.00      0.00         1\n",
            "        1959       1.00      0.75      0.86         4\n",
            "        1960       0.00      0.00      0.00         1\n",
            "        1961       0.00      0.00      0.00         1\n",
            "        1967       0.24      0.47      0.32        19\n",
            "        1968       0.28      0.50      0.36        10\n",
            "        1969       0.00      0.00      0.00         2\n",
            "        1970       0.50      0.50      0.50         2\n",
            "        1971       0.00      0.00      0.00         2\n",
            "        1972       1.00      1.00      1.00         1\n",
            "        1974       1.00      1.00      1.00         3\n",
            "        1975       0.50      0.33      0.40         6\n",
            "        1976       1.00      1.00      1.00         1\n",
            "        1979       0.00      0.00      0.00         3\n",
            "        1980       0.00      0.00      0.00         1\n",
            "        1982       0.00      0.00      0.00         1\n",
            "        1988       0.00      0.00      0.00         1\n",
            "        1989       1.00      0.33      0.50         3\n",
            "        1990       0.42      0.58      0.49        24\n",
            "        1991       0.80      1.00      0.89         4\n",
            "        1992       0.24      0.58      0.34        33\n",
            "        1993       0.43      0.38      0.40         8\n",
            "        1994       0.39      0.70      0.50        20\n",
            "        1998       1.00      1.00      1.00         3\n",
            "        1999       1.00      0.67      0.80         3\n",
            "        2000       0.00      0.00      0.00         6\n",
            "        2001       0.00      0.00      0.00         2\n",
            "        2002       0.00      0.00      0.00         2\n",
            "        2003       0.00      0.00      0.00         1\n",
            "        2005       0.00      0.00      0.00         1\n",
            "        2006       0.00      0.00      0.00         1\n",
            "        2008       0.00      0.00      0.00         1\n",
            "        2010       0.00      0.00      0.00         1\n",
            "        2011       0.00      0.00      0.00         1\n",
            "        2014       0.00      0.00      0.00         1\n",
            "        2024       0.00      0.00      0.00         6\n",
            "        2025       0.00      0.00      0.00         5\n",
            "        2026       0.83      0.31      0.45        16\n",
            "        2027       0.00      0.00      0.00         1\n",
            "        2028       0.00      0.00      0.00         2\n",
            "        2030       0.00      0.00      0.00         2\n",
            "        2031       1.00      0.50      0.67         2\n",
            "        2032       1.00      1.00      1.00         1\n",
            "        2033       1.00      0.50      0.67         2\n",
            "        2034       0.00      0.00      0.00         1\n",
            "        2035       0.00      0.00      0.00         2\n",
            "        2039       0.00      0.00      0.00         1\n",
            "        2040       0.50      1.00      0.67         1\n",
            "        2041       0.00      0.00      0.00         1\n",
            "        2042       0.00      0.00      0.00         1\n",
            "        2047       1.00      0.50      0.67         2\n",
            "        2049       0.00      0.00      0.00         2\n",
            "        2050       0.00      0.00      0.00         1\n",
            "        2052       1.00      0.50      0.67         2\n",
            "        2054       1.00      1.00      1.00         1\n",
            "        2055       0.00      0.00      0.00         1\n",
            "        2056       0.00      0.00      0.00         1\n",
            "        2061       1.00      1.00      1.00         1\n",
            "        2062       0.00      0.00      0.00         1\n",
            "        2071       0.50      0.25      0.33        16\n",
            "        2072       0.26      0.28      0.27        18\n",
            "        2073       1.00      1.00      1.00         1\n",
            "        2074       0.40      0.33      0.36         6\n",
            "        2075       0.00      0.00      0.00         1\n",
            "        2077       0.00      0.00      0.00         1\n",
            "        2078       0.62      0.75      0.68        20\n",
            "        2079       0.52      0.65      0.58        20\n",
            "        2080       0.00      0.00      0.00         2\n",
            "        2081       1.00      0.50      0.67         2\n",
            "        2082       0.00      0.00      0.00         1\n",
            "        2083       1.00      0.25      0.40         4\n",
            "        2084       1.00      1.00      1.00         4\n",
            "        2085       0.67      1.00      0.80         2\n",
            "        2086       0.00      0.00      0.00         1\n",
            "        2087       0.00      0.00      0.00         1\n",
            "        2095       0.00      0.00      0.00         1\n",
            "        2097       0.00      0.00      0.00         1\n",
            "        2108       0.42      0.78      0.55        18\n",
            "        2109       0.50      0.40      0.44        15\n",
            "        2110       0.31      0.50      0.38        32\n",
            "        2111       0.50      0.29      0.37        17\n",
            "        2112       1.00      0.33      0.50         6\n",
            "        2113       0.50      0.33      0.40         3\n",
            "        2114       1.00      0.25      0.40         4\n",
            "        2115       1.00      0.33      0.50         3\n",
            "        2117       1.00      0.50      0.67         2\n",
            "        2118       0.00      0.00      0.00         2\n",
            "        2121       0.00      0.00      0.00         3\n",
            "        2122       0.00      0.00      0.00         1\n",
            "        2123       0.00      0.00      0.00         2\n",
            "        2124       0.00      0.00      0.00         6\n",
            "        2125       0.00      0.00      0.00         1\n",
            "        2127       0.00      0.00      0.00         1\n",
            "        2128       1.00      0.50      0.67         2\n",
            "        2129       0.00      0.00      0.00         3\n",
            "        2139       0.00      0.00      0.00         1\n",
            "        2140       0.00      0.00      0.00         1\n",
            "        2143       0.00      0.00      0.00         1\n",
            "        2145       1.00      1.00      1.00         1\n",
            "        2147       1.00      1.00      1.00         1\n",
            "        2151       1.00      1.00      1.00         1\n",
            "        2152       0.00      0.00      0.00         1\n",
            "        2153       0.00      0.00      0.00         1\n",
            "        2154       0.00      0.00      0.00         1\n",
            "        2155       0.00      0.00      0.00         1\n",
            "        2156       1.00      1.00      1.00         1\n",
            "        2157       0.00      0.00      0.00         1\n",
            "        2158       0.00      0.00      0.00         6\n",
            "        2159       1.00      0.17      0.29         6\n",
            "        2160       0.00      0.00      0.00         6\n",
            "        2162       1.00      0.33      0.50         3\n",
            "        2163       1.00      1.00      1.00         1\n",
            "        2164       1.00      1.00      1.00         1\n",
            "        2165       0.00      0.00      0.00         1\n",
            "        2166       0.00      0.00      0.00         1\n",
            "        2169       1.00      1.00      1.00         1\n",
            "        2170       0.00      0.00      0.00         1\n",
            "        2173       1.00      1.00      1.00         1\n",
            "        2174       1.00      1.00      1.00         1\n",
            "        2178       1.00      1.00      1.00         1\n",
            "        2182       0.00      0.00      0.00         1\n",
            "        2183       0.00      0.00      0.00         1\n",
            "        2185       1.00      0.50      0.67         2\n",
            "        2186       0.00      0.00      0.00         4\n",
            "        2187       0.00      0.00      0.00         2\n",
            "        2189       0.50      0.25      0.33         4\n",
            "        2190       0.50      0.50      0.50        10\n",
            "        2191       0.00      0.00      0.00         1\n",
            "        2195       0.78      0.82      0.80        22\n",
            "        2197       1.00      0.50      0.67         2\n",
            "        2198       1.00      0.67      0.80         3\n",
            "        2201       0.57      0.59      0.58        29\n",
            "        2202       0.33      0.50      0.40        18\n",
            "        2203       0.40      0.62      0.48        13\n",
            "        2204       0.80      0.40      0.53        10\n",
            "        2205       0.50      0.60      0.55         5\n",
            "        2206       0.40      0.33      0.36         6\n",
            "        2208       1.00      0.67      0.80         3\n",
            "        2212       1.00      0.33      0.50         3\n",
            "        2214       1.00      1.00      1.00         1\n",
            "        2215       1.00      0.50      0.67         2\n",
            "        2216       0.00      0.00      0.00         1\n",
            "        2217       0.00      0.00      0.00         2\n",
            "        2218       1.00      1.00      1.00         1\n",
            "        2221       0.62      0.36      0.45        14\n",
            "        2222       0.80      0.57      0.67        14\n",
            "        2225       1.00      1.00      1.00         2\n",
            "        2226       0.00      0.00      0.00         4\n",
            "        2227       0.00      0.00      0.00         1\n",
            "        2228       0.00      0.00      0.00         2\n",
            "        2231       0.00      0.00      0.00         2\n",
            "        2233       0.00      0.00      0.00         1\n",
            "        2237       0.00      0.00      0.00         1\n",
            "        2243       1.00      1.00      1.00         3\n",
            "        2244       1.00      0.50      0.67         2\n",
            "        2245       1.00      1.00      1.00         2\n",
            "        2246       1.00      1.00      1.00         2\n",
            "        2247       0.00      0.00      0.00         4\n",
            "        2248       1.00      1.00      1.00         2\n",
            "        2249       1.00      1.00      1.00         1\n",
            "        2251       1.00      1.00      1.00         1\n",
            "        2253       1.00      0.25      0.40         4\n",
            "        2254       0.00      0.00      0.00         1\n",
            "        2256       1.00      0.88      0.93        16\n",
            "        2257       0.46      0.87      0.60        15\n",
            "        2258       1.00      1.00      1.00        13\n",
            "        2259       1.00      1.00      1.00         3\n",
            "        2260       1.00      0.89      0.94         9\n",
            "        2261       1.00      1.00      1.00         3\n",
            "        2264       1.00      1.00      1.00         1\n",
            "        2265       1.00      1.00      1.00         1\n",
            "        2266       1.00      1.00      1.00         1\n",
            "        2267       1.00      1.00      1.00         1\n",
            "        2268       0.00      0.00      0.00         1\n",
            "        2269       1.00      0.50      0.67         2\n",
            "        2270       1.00      0.50      0.67         2\n",
            "        2271       0.25      0.50      0.33         2\n",
            "        2272       0.00      0.00      0.00         1\n",
            "        2273       0.00      0.00      0.00         1\n",
            "        2276       1.00      1.00      1.00         1\n",
            "        2277       1.00      1.00      1.00         1\n",
            "        2278       0.00      0.00      0.00         1\n",
            "        2280       0.00      0.00      0.00         1\n",
            "        2282       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.47      3716\n",
            "   macro avg       0.42      0.37      0.38      3716\n",
            "weighted avg       0.46      0.47      0.43      3716\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "res = confusion_matrix(Y_labels, predictions2)\n",
        "for i in range(0,len(res)):\n",
        "  print(res[i])\n",
        "print(\"Confusion Matrix\")\n",
        "print(res)\n",
        "print(f\"Test Set: {len(Y_test)}\")\n",
        "print(f\"Accuracy = {score_test*100} %\")\n",
        "print(classification_report(Y_labels, predictions2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqopa7evQlTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}